diff --git a/arguments/__init__.py b/arguments/__init__.py
index 1e13a55..4dbccfb 100644
--- a/arguments/__init__.py
+++ b/arguments/__init__.py
@@ -63,6 +63,7 @@ class ModelParams(ParamGroup):
 
 class PipelineParams(ParamGroup):
     def __init__(self, parser):
+        self.separate_sh = True
         self.convert_SHs_python = False
         self.compute_cov3D_python = False
         self.debug = False
@@ -76,7 +77,7 @@ class OptimizationParams(ParamGroup):
         self.position_lr_delay_mult = 0.01
         self.position_lr_max_steps = 30_000
         self.feature_lr = 0.0025
-        self.opacity_lr = 0.05
+        self.opacity_lr = 0.025
         self.scaling_lr = 0.005
         self.rotation_lr = 0.001
         self.percent_dense = 0.01
@@ -87,6 +88,7 @@ class OptimizationParams(ParamGroup):
         self.densify_until_iter = 15_000
         self.densify_grad_threshold = 0.0002
         self.random_background = False
+        self.optimizer_type = "sparse_adam"
         super().__init__(parser, "Optimization Parameters")
 
 def get_combined_args(parser : ArgumentParser):
diff --git a/environment.yml b/environment.yml
index d479ec7..40f700c 100644
--- a/environment.yml
+++ b/environment.yml
@@ -1,4 +1,4 @@
-name: gaussian_splatting
+name: taming_3dgs
 channels:
   - pytorch
   - conda-forge
diff --git a/gaussian_renderer/__init__.py b/gaussian_renderer/__init__.py
index f74e336..27d027a 100644
--- a/gaussian_renderer/__init__.py
+++ b/gaussian_renderer/__init__.py
@@ -59,6 +59,7 @@ def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor,
     scales = None
     rotations = None
     cov3D_precomp = None
+
     if pipe.compute_cov3D_python:
         cov3D_precomp = pc.get_covariance(scaling_modifier)
     else:
@@ -77,24 +78,39 @@ def render(viewpoint_camera, pc : GaussianModel, pipe, bg_color : torch.Tensor,
             sh2rgb = eval_sh(pc.active_sh_degree, shs_view, dir_pp_normalized)
             colors_precomp = torch.clamp_min(sh2rgb + 0.5, 0.0)
         else:
-            shs = pc.get_features
+            if pipe.separate_sh:
+                dc, shs = pc.get_features_dc, pc.get_features_rest
+            else:
+                shs = pc.get_features
     else:
         colors_precomp = override_color
 
     # Rasterize visible Gaussians to image, obtain their radii (on screen). 
-    rendered_image, radii = rasterizer(
-        means3D = means3D,
-        means2D = means2D,
-        shs = shs,
-        colors_precomp = colors_precomp,
-        opacities = opacity,
-        scales = scales,
-        rotations = rotations,
-        cov3D_precomp = cov3D_precomp)
+    if pipe.separate_sh:
+        rendered_image, radii = rasterizer(
+            means3D = means3D,
+            means2D = means2D,
+            dc = dc,
+            shs = shs,
+            colors_precomp = colors_precomp,
+            opacities = opacity,
+            scales = scales,
+            rotations = rotations,
+            cov3D_precomp = cov3D_precomp)
+    else:
+        rendered_image, radii = rasterizer(
+            means3D = means3D,
+            means2D = means2D,
+            shs = shs,
+            colors_precomp = colors_precomp,
+            opacities = opacity,
+            scales = scales,
+            rotations = rotations,
+            cov3D_precomp = cov3D_precomp)
 
     # Those Gaussians that were frustum culled or had a radius of 0 were not visible.
     # They will be excluded from value updates used in the splitting criteria.
     return {"render": rendered_image,
             "viewspace_points": screenspace_points,
-            "visibility_filter" : radii > 0,
+            "visibility_filter" : (radii > 0).nonzero(),
             "radii": radii}
diff --git a/scene/gaussian_model.py b/scene/gaussian_model.py
index 632a1e8..4641562 100644
--- a/scene/gaussian_model.py
+++ b/scene/gaussian_model.py
@@ -21,6 +21,11 @@ from simple_knn._C import distCUDA2
 from utils.graphics_utils import BasicPointCloud
 from utils.general_utils import strip_symmetric, build_scaling_rotation
 
+try:
+    from diff_gaussian_rasterization import SparseGaussianAdam
+except:
+    pass
+
 class GaussianModel:
 
     def setup_functions(self):
@@ -41,8 +46,9 @@ class GaussianModel:
         self.rotation_activation = torch.nn.functional.normalize
 
 
-    def __init__(self, sh_degree : int):
+    def __init__(self, sh_degree, optimizer_type="default"):
         self.active_sh_degree = 0
+        self.optimizer_type = optimizer_type
         self.max_sh_degree = sh_degree  
         self._xyz = torch.empty(0)
         self._features_dc = torch.empty(0)
@@ -110,6 +116,14 @@ class GaussianModel:
         features_rest = self._features_rest
         return torch.cat((features_dc, features_rest), dim=1)
     
+    @property
+    def get_features_dc(self):
+        return self._features_dc
+    
+    @property
+    def get_features_rest(self):
+        return self._features_rest
+    
     @property
     def get_opacity(self):
         return self.opacity_activation(self._opacity)
@@ -136,7 +150,7 @@ class GaussianModel:
         rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
         rots[:, 0] = 1
 
-        opacities = inverse_sigmoid(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))
+        opacities = self.inverse_opacity_activation(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))
 
         self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))
         self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))
@@ -160,7 +174,10 @@ class GaussianModel:
             {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"}
         ]
 
-        self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)
+        if self.optimizer_type == "default":
+            self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)
+        elif self.optimizer_type == "sparse_adam":
+            self.optimizer = SparseGaussianAdam(l, lr=0.0, eps=1e-15)
         self.xyz_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,
                                                     lr_final=training_args.position_lr_final*self.spatial_lr_scale,
                                                     lr_delay_mult=training_args.position_lr_delay_mult,
@@ -208,7 +225,7 @@ class GaussianModel:
         PlyData([el]).write(path)
 
     def reset_opacity(self):
-        opacities_new = inverse_sigmoid(torch.min(self.get_opacity, torch.ones_like(self.get_opacity)*0.01))
+        opacities_new = self.inverse_opacity_activation(torch.min(self.get_opacity, torch.ones_like(self.get_opacity)*0.01))
         optimizable_tensors = self.replace_tensor_to_optimizer(opacities_new, "opacity")
         self._opacity = optimizable_tensors["opacity"]
 
@@ -303,6 +320,7 @@ class GaussianModel:
 
         self.denom = self.denom[valid_points_mask]
         self.max_radii2D = self.max_radii2D[valid_points_mask]
+        self.tmp_radii = self.tmp_radii[valid_points_mask]
 
     def cat_tensors_to_optimizer(self, tensors_dict):
         optimizable_tensors = {}
@@ -326,7 +344,7 @@ class GaussianModel:
 
         return optimizable_tensors
 
-    def densification_postfix(self, new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation):
+    def densification_postfix(self, new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation, new_tmp_radii):
         d = {"xyz": new_xyz,
         "f_dc": new_features_dc,
         "f_rest": new_features_rest,
@@ -342,6 +360,7 @@ class GaussianModel:
         self._scaling = optimizable_tensors["scaling"]
         self._rotation = optimizable_tensors["rotation"]
 
+        self.tmp_radii = torch.cat((self.tmp_radii, new_tmp_radii))
         self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")
         self.denom = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")
         self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device="cuda")
@@ -365,8 +384,9 @@ class GaussianModel:
         new_features_dc = self._features_dc[selected_pts_mask].repeat(N,1,1)
         new_features_rest = self._features_rest[selected_pts_mask].repeat(N,1,1)
         new_opacity = self._opacity[selected_pts_mask].repeat(N,1)
+        new_tmp_radii = self.tmp_radii[selected_pts_mask].repeat(N)
 
-        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacity, new_scaling, new_rotation)
+        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacity, new_scaling, new_rotation, new_tmp_radii)
 
         prune_filter = torch.cat((selected_pts_mask, torch.zeros(N * selected_pts_mask.sum(), device="cuda", dtype=bool)))
         self.prune_points(prune_filter)
@@ -384,12 +404,15 @@ class GaussianModel:
         new_scaling = self._scaling[selected_pts_mask]
         new_rotation = self._rotation[selected_pts_mask]
 
-        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation)
+        new_tmp_radii = self.tmp_radii[selected_pts_mask]
+
+        self.densification_postfix(new_xyz, new_features_dc, new_features_rest, new_opacities, new_scaling, new_rotation, new_tmp_radii)
 
-    def densify_and_prune(self, max_grad, min_opacity, extent, max_screen_size):
+    def densify_and_prune(self, max_grad, min_opacity, extent, max_screen_size, radii):
         grads = self.xyz_gradient_accum / self.denom
         grads[grads.isnan()] = 0.0
 
+        self.tmp_radii = radii
         self.densify_and_clone(grads, max_grad, extent)
         self.densify_and_split(grads, max_grad, extent)
 
@@ -399,6 +422,8 @@ class GaussianModel:
             big_points_ws = self.get_scaling.max(dim=1).values > 0.1 * extent
             prune_mask = torch.logical_or(torch.logical_or(prune_mask, big_points_vs), big_points_ws)
         self.prune_points(prune_mask)
+        tmp_radii = self.tmp_radii
+        self.tmp_radii = None
 
         torch.cuda.empty_cache()
 
diff --git a/submodules/diff-gaussian-rasterization/conv.cu b/submodules/diff-gaussian-rasterization/conv.cu
new file mode 100644
index 0000000..4270539
--- /dev/null
+++ b/submodules/diff-gaussian-rasterization/conv.cu
@@ -0,0 +1,1195 @@
+#include <torch/extension.h>
+#include <cooperative_groups.h>
+#include <algorithm>
+#include <iostream>
+
+namespace cg = cooperative_groups;
+
+#define G_00 0.001028380123898387f
+#define G_01 0.0075987582094967365f
+#define G_02 0.036000773310661316f
+#define G_03 0.10936068743467331f
+#define G_04 0.21300552785396576f
+#define G_05 0.26601171493530273f
+#define G_06 0.21300552785396576f
+#define G_07 0.10936068743467331f
+#define G_08 0.036000773310661316f
+#define G_09 0.0075987582094967365f
+#define G_10 0.001028380123898387f
+
+#define G_000 0.0000010576f
+#define G_001 0.0000078144f
+#define G_002 0.0000370225f
+#define G_003 0.0001124644f
+#define G_004 0.0002190506f
+#define G_005 0.0002735612f
+#define G_006 0.0002190506f
+#define G_007 0.0001124644f
+#define G_008 0.0000370225f
+#define G_009 0.0000078144f
+#define G_010 0.0000010576f
+#define G_011 0.0000078144f
+#define G_012 0.0000577411f
+#define G_013 0.0002735612f
+#define G_014 0.0008310054f
+#define G_015 0.0016185775f
+#define G_016 0.0020213588f
+#define G_017 0.0016185775f
+#define G_018 0.0008310054f
+#define G_019 0.0002735612f
+#define G_020 0.0000577411f
+#define G_021 0.0000078144f
+#define G_022 0.0000370225f
+#define G_023 0.0002735612f
+#define G_024 0.0012960557f
+#define G_025 0.0039370693f
+#define G_026 0.0076683639f
+#define G_027 0.0095766271f
+#define G_028 0.0076683639f
+#define G_029 0.0039370693f
+#define G_030 0.0012960557f
+#define G_031 0.0002735612f
+#define G_032 0.0000370225f
+#define G_033 0.0001124644f
+#define G_034 0.0008310054f
+#define G_035 0.0039370693f
+#define G_036 0.0119597595f
+#define G_037 0.0232944302f
+#define G_038 0.0290912241f
+#define G_039 0.0232944302f
+#define G_040 0.0119597595f
+#define G_041 0.0039370693f
+#define G_042 0.0008310054f
+#define G_043 0.0001124644f
+#define G_044 0.0002190506f
+#define G_045 0.0016185775f
+#define G_046 0.0076683639f
+#define G_047 0.0232944302f
+#define G_048 0.0453713536f
+#define G_049 0.0566619672f
+#define G_050 0.0453713536f
+#define G_051 0.0232944302f
+#define G_052 0.0076683639f
+#define G_053 0.0016185775f
+#define G_054 0.0002190506f
+#define G_055 0.0002735612f
+#define G_056 0.0020213588f
+#define G_057 0.0095766271f
+#define G_058 0.0290912241f
+#define G_059 0.0566619672f
+#define G_060 0.0707622319f
+#define G_061 0.0566619672f
+#define G_062 0.0290912241f
+#define G_063 0.0095766271f
+#define G_064 0.0020213588f
+#define G_065 0.0002735612f
+#define G_066 0.0002190506f
+#define G_067 0.0016185775f
+#define G_068 0.0076683639f
+#define G_069 0.0232944302f
+#define G_070 0.0453713536f
+#define G_071 0.0566619672f
+#define G_072 0.0453713536f
+#define G_073 0.0232944302f
+#define G_074 0.0076683639f
+#define G_075 0.0016185775f
+#define G_076 0.0002190506f
+#define G_077 0.0001124644f
+#define G_078 0.0008310054f
+#define G_079 0.0039370693f
+#define G_080 0.0119597595f
+#define G_081 0.0232944302f
+#define G_082 0.0290912241f
+#define G_083 0.0232944302f
+#define G_084 0.0119597595f
+#define G_085 0.0039370693f
+#define G_086 0.0008310054f
+#define G_087 0.0001124644f
+#define G_088 0.0000370225f
+#define G_089 0.0002735612f
+#define G_090 0.0012960557f
+#define G_091 0.0039370693f
+#define G_092 0.0076683639f
+#define G_093 0.0095766271f
+#define G_094 0.0076683639f
+#define G_095 0.0039370693f
+#define G_096 0.0012960557f
+#define G_097 0.0002735612f
+#define G_098 0.0000370225f
+#define G_099 0.0000078144f
+#define G_100 0.0000577411f
+#define G_101 0.0002735612f
+#define G_102 0.0008310054f
+#define G_103 0.0016185775f
+#define G_104 0.0020213588f
+#define G_105 0.0016185775f
+#define G_106 0.0008310054f
+#define G_107 0.0002735612f
+#define G_108 0.0000577411f
+#define G_109 0.0000078144f
+#define G_110 0.0000010576f
+#define G_111 0.0000078144f
+#define G_112 0.0000370225f
+#define G_113 0.0001124644f
+#define G_114 0.0002190506f
+#define G_115 0.0002735612f
+#define G_116 0.0002190506f
+#define G_117 0.0001124644f
+#define G_118 0.0000370225f
+#define G_119 0.0000078144f
+#define G_120 0.0000010576f
+
+#define BX 32
+#define BY 32
+#define BLOCK_DIM 16
+
+
+template <int C>
+__device__ float get_pix_value(const float* img, const int c, const int y, const int x, const int H, const int W) {
+    if (x >= W || y >= H || x < 0 || y < 0) {
+        return 0.0f;
+    } else {
+        return img[c * H * W + y * W + x];
+    }
+}
+
+/*
+ * Copyright 1993-2007 NVIDIA Corporation.  All rights reserved.
+ *
+ * NOTICE TO USER:
+ *
+ * This source code is subject to NVIDIA ownership rights under U.S. and
+ * international Copyright laws.  Users and possessors of this source code
+ * are hereby granted a nonexclusive, royalty-free license to use this code
+ * in individual and commercial software.
+ *
+ * NVIDIA MAKES NO REPRESENTATION ABOUT THE SUITABILITY OF THIS SOURCE
+ * CODE FOR ANY PURPOSE.  IT IS PROVIDED "AS IS" WITHOUT EXPRESS OR
+ * IMPLIED WARRANTY OF ANY KIND.  NVIDIA DISCLAIMS ALL WARRANTIES WITH
+ * REGARD TO THIS SOURCE CODE, INCLUDING ALL IMPLIED WARRANTIES OF
+ * MERCHANTABILITY, NONINFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.
+ * IN NO EVENT SHALL NVIDIA BE LIABLE FOR ANY SPECIAL, INDIRECT, INCIDENTAL,
+ * OR CONSEQUENTIAL DAMAGES, OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
+ * OF USE, DATA OR PROFITS,  WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE
+ * OR OTHER TORTIOUS ACTION,  ARISING OUT OF OR IN CONNECTION WITH THE USE
+ * OR PERFORMANCE OF THIS SOURCE CODE.
+ *
+ * U.S. Government End Users.   This source code is a "commercial item" as
+ * that term is defined at  48 C.F.R. 2.numIterations1 (OCT 1995), consisting  of
+ * "commercial computer  software"  and "commercial computer software
+ * documentation" as such terms are  used in 48 C.F.R. 12.212 (SEPT 1995)
+ * and is provided to the U.S. Government only as a commercial end item.
+ * Consistent with 48 C.F.R.12.212 and 48 C.F.R. 227.7202-1 through
+ * 227.7202-4 (JUNE 1995), all U.S. Government End Users acquire the
+ * source code with only those rights set forth herein.
+ *
+ * Any use of this source code in individual and commercial software must
+ * include, in the user documentation and internal comments to the code,
+ * the above Disclaimer and U.S. Government End Users Notice.
+ */
+template<int C>
+__global__ void transposeCUDA(float *odata, float *idata, int width, int height)
+{
+	__shared__ float block[BLOCK_DIM][BLOCK_DIM+1];
+    const int num_pix = width * height;
+	
+    for (int c = 0; c < C; ++c) {
+        unsigned int xIndex = blockIdx.x * BLOCK_DIM + threadIdx.x;
+        unsigned int yIndex = blockIdx.y * BLOCK_DIM + threadIdx.y;
+        if((xIndex < width) && (yIndex < height))
+        {
+            unsigned int index_in = yIndex * width + xIndex;
+            block[threadIdx.y][threadIdx.x] = idata[num_pix * c + index_in];
+        }
+
+        __syncthreads();
+
+        xIndex = blockIdx.y * BLOCK_DIM + threadIdx.x;
+        yIndex = blockIdx.x * BLOCK_DIM + threadIdx.y;
+        if((xIndex < height) && (yIndex < width))
+        {
+            unsigned int index_out = yIndex * height + xIndex;
+            odata[num_pix * c + index_out] = block[threadIdx.x][threadIdx.y];
+        }
+        __syncthreads();
+    }
+}
+
+template <int C>
+__global__ void separableConvCUDA(
+    const float* __restrict__ input,
+    float* __restrict__ output,
+    const int H,
+    const int W)
+{
+	auto block = cg::this_thread_block();
+    const int pix_y = block.group_index().y * block.dim_threads().y + block.thread_index().y;
+    const int pix_x = block.group_index().x * block.dim_threads().x + block.thread_index().x;
+    const int pix_id = pix_y * W + pix_x;
+    const int num_pix = H * W;
+
+    __shared__ float pixels[BY][BX + 10];
+    const int start_y = block.group_index().y * block.dim_threads().y;
+    const int start_x = block.group_index().x * block.dim_threads().x;
+    
+    const int cnt = BY * (BX + 10);
+    const int num_blocks = (cnt + BX * BY - 1) / (BX * BY);
+
+    for (int i = 0; i < C; ++i) {
+
+        for (int b = 0; b < num_blocks; ++b) {
+            int tid = b * (BX * BY) + block.thread_rank();
+            if (tid < cnt) {
+                int local_y = tid / (BX + 10);
+                int local_x = tid % (BX + 10);
+                int y = start_y + local_y;
+                int x = start_x + local_x;
+                pixels[local_y][local_x] = get_pix_value<C>(input, i, y, x - 5, H, W);
+            }
+        }
+        block.sync();
+
+        if (pix_x < W && pix_y < H) {
+            int local_y = block.thread_index().y;
+            int local_x = block.thread_index().x + 5;
+            float val = 0.0f;
+            val += G_00 * pixels[local_y][local_x - 5];
+            val += G_01 * pixels[local_y][local_x - 4];
+            val += G_02 * pixels[local_y][local_x - 3];
+            val += G_03 * pixels[local_y][local_x - 2];
+            val += G_04 * pixels[local_y][local_x - 1];
+            val += G_05 * pixels[local_y][local_x    ];
+            val += G_06 * pixels[local_y][local_x + 1];
+            val += G_07 * pixels[local_y][local_x + 2];
+            val += G_08 * pixels[local_y][local_x + 3];
+            val += G_09 * pixels[local_y][local_x + 4];
+            val += G_10 * pixels[local_y][local_x + 5];
+            output[i * num_pix + pix_id] = val;
+        }
+        block.sync();
+    }
+}
+
+template <int C>
+__global__ void convCUDA(
+    const float* __restrict__ input,
+    float* __restrict__ output,
+    const int H,
+    const int W)
+{
+	auto block = cg::this_thread_block();
+    const int pix_y = block.group_index().y * block.dim_threads().y + block.thread_index().y;
+    const int pix_x = block.group_index().x * block.dim_threads().x + block.thread_index().x;
+    const int pix_id = pix_y * W + pix_x;
+    const int num_pix = H * W;
+
+    __shared__ float pixels[BY + 10][BX + 10];
+    const int start_y = block.group_index().y * block.dim_threads().y;
+    const int start_x = block.group_index().x * block.dim_threads().x;
+    
+    const int cnt = (BY + 10) * (BX + 10);
+    const int num_blocks = (cnt + BX * BY - 1) / (BX * BY);
+
+    for (int i = 0; i < C; ++i) {
+
+        for (int b = 0; b < num_blocks; ++b) {
+            int tid = b * (BX * BY) + block.thread_rank();
+            if (tid < cnt) {
+                int local_y = tid / (BX + 10);
+                int local_x = tid % (BX + 10);
+                int y = start_y + local_y;
+                int x = start_x + local_x;
+                pixels[local_y][local_x] = get_pix_value<C>(input, i, y - 5, x - 5, H, W);
+            }
+        }
+        block.sync();
+
+        if (pix_x < W && pix_y < H) {
+            int local_y = block.thread_index().y + 5;
+            int local_x = block.thread_index().x + 5;
+            float val = 0.0f;
+
+            {
+                val += G_000 * pixels[local_y - 5][local_x - 5];
+                val += G_001 * pixels[local_y - 5][local_x - 4];
+                val += G_002 * pixels[local_y - 5][local_x - 3];
+                val += G_003 * pixels[local_y - 5][local_x - 2];
+                val += G_004 * pixels[local_y - 5][local_x - 1];
+                val += G_005 * pixels[local_y - 5][local_x    ];
+                val += G_006 * pixels[local_y - 5][local_x + 1];
+                val += G_007 * pixels[local_y - 5][local_x + 2];
+                val += G_008 * pixels[local_y - 5][local_x + 3];
+                val += G_009 * pixels[local_y - 5][local_x + 4];
+                val += G_010 * pixels[local_y - 5][local_x + 5];
+                val += G_011 * pixels[local_y - 4][local_x - 5];
+                val += G_012 * pixels[local_y - 4][local_x - 4];
+                val += G_013 * pixels[local_y - 4][local_x - 3];
+                val += G_014 * pixels[local_y - 4][local_x - 2];
+                val += G_015 * pixels[local_y - 4][local_x - 1];
+                val += G_016 * pixels[local_y - 4][local_x    ];
+                val += G_017 * pixels[local_y - 4][local_x + 1];
+                val += G_018 * pixels[local_y - 4][local_x + 2];
+                val += G_019 * pixels[local_y - 4][local_x + 3];
+                val += G_020 * pixels[local_y - 4][local_x + 4];
+                val += G_021 * pixels[local_y - 4][local_x + 5];
+                val += G_022 * pixels[local_y - 3][local_x - 5];
+                val += G_023 * pixels[local_y - 3][local_x - 4];
+                val += G_024 * pixels[local_y - 3][local_x - 3];
+                val += G_025 * pixels[local_y - 3][local_x - 2];
+                val += G_026 * pixels[local_y - 3][local_x - 1];
+                val += G_027 * pixels[local_y - 3][local_x    ];
+                val += G_028 * pixels[local_y - 3][local_x + 1];
+                val += G_029 * pixels[local_y - 3][local_x + 2];
+                val += G_030 * pixels[local_y - 3][local_x + 3];
+                val += G_031 * pixels[local_y - 3][local_x + 4];
+                val += G_032 * pixels[local_y - 3][local_x + 5];
+                val += G_033 * pixels[local_y - 2][local_x - 5];
+                val += G_034 * pixels[local_y - 2][local_x - 4];
+                val += G_035 * pixels[local_y - 2][local_x - 3];
+                val += G_036 * pixels[local_y - 2][local_x - 2];
+                val += G_037 * pixels[local_y - 2][local_x - 1];
+                val += G_038 * pixels[local_y - 2][local_x    ];
+                val += G_039 * pixels[local_y - 2][local_x + 1];
+                val += G_040 * pixels[local_y - 2][local_x + 2];
+                val += G_041 * pixels[local_y - 2][local_x + 3];
+                val += G_042 * pixels[local_y - 2][local_x + 4];
+                val += G_043 * pixels[local_y - 2][local_x + 5];
+                val += G_044 * pixels[local_y - 1][local_x - 5];
+                val += G_045 * pixels[local_y - 1][local_x - 4];
+                val += G_046 * pixels[local_y - 1][local_x - 3];
+                val += G_047 * pixels[local_y - 1][local_x - 2];
+                val += G_048 * pixels[local_y - 1][local_x - 1];
+                val += G_049 * pixels[local_y - 1][local_x    ];
+                val += G_050 * pixels[local_y - 1][local_x + 1];
+                val += G_051 * pixels[local_y - 1][local_x + 2];
+                val += G_052 * pixels[local_y - 1][local_x + 3];
+                val += G_053 * pixels[local_y - 1][local_x + 4];
+                val += G_054 * pixels[local_y - 1][local_x + 5];
+                val += G_055 * pixels[local_y    ][local_x - 5];
+                val += G_056 * pixels[local_y    ][local_x - 4];
+                val += G_057 * pixels[local_y    ][local_x - 3];
+                val += G_058 * pixels[local_y    ][local_x - 2];
+                val += G_059 * pixels[local_y    ][local_x - 1];
+                val += G_060 * pixels[local_y    ][local_x    ];
+                val += G_061 * pixels[local_y    ][local_x + 1];
+                val += G_062 * pixels[local_y    ][local_x + 2];
+                val += G_063 * pixels[local_y    ][local_x + 3];
+                val += G_064 * pixels[local_y    ][local_x + 4];
+                val += G_065 * pixels[local_y    ][local_x + 5];
+                val += G_066 * pixels[local_y + 1][local_x - 5];
+                val += G_067 * pixels[local_y + 1][local_x - 4];
+                val += G_068 * pixels[local_y + 1][local_x - 3];
+                val += G_069 * pixels[local_y + 1][local_x - 2];
+                val += G_070 * pixels[local_y + 1][local_x - 1];
+                val += G_071 * pixels[local_y + 1][local_x    ];
+                val += G_072 * pixels[local_y + 1][local_x + 1];
+                val += G_073 * pixels[local_y + 1][local_x + 2];
+                val += G_074 * pixels[local_y + 1][local_x + 3];
+                val += G_075 * pixels[local_y + 1][local_x + 4];
+                val += G_076 * pixels[local_y + 1][local_x + 5];
+                val += G_077 * pixels[local_y + 2][local_x - 5];
+                val += G_078 * pixels[local_y + 2][local_x - 4];
+                val += G_079 * pixels[local_y + 2][local_x - 3];
+                val += G_080 * pixels[local_y + 2][local_x - 2];
+                val += G_081 * pixels[local_y + 2][local_x - 1];
+                val += G_082 * pixels[local_y + 2][local_x    ];
+                val += G_083 * pixels[local_y + 2][local_x + 1];
+                val += G_084 * pixels[local_y + 2][local_x + 2];
+                val += G_085 * pixels[local_y + 2][local_x + 3];
+                val += G_086 * pixels[local_y + 2][local_x + 4];
+                val += G_087 * pixels[local_y + 2][local_x + 5];
+                val += G_088 * pixels[local_y + 3][local_x - 5];
+                val += G_089 * pixels[local_y + 3][local_x - 4];
+                val += G_090 * pixels[local_y + 3][local_x - 3];
+                val += G_091 * pixels[local_y + 3][local_x - 2];
+                val += G_092 * pixels[local_y + 3][local_x - 1];
+                val += G_093 * pixels[local_y + 3][local_x    ];
+                val += G_094 * pixels[local_y + 3][local_x + 1];
+                val += G_095 * pixels[local_y + 3][local_x + 2];
+                val += G_096 * pixels[local_y + 3][local_x + 3];
+                val += G_097 * pixels[local_y + 3][local_x + 4];
+                val += G_098 * pixels[local_y + 3][local_x + 5];
+                val += G_099 * pixels[local_y + 4][local_x - 5];
+                val += G_100 * pixels[local_y + 4][local_x - 4];
+                val += G_101 * pixels[local_y + 4][local_x - 3];
+                val += G_102 * pixels[local_y + 4][local_x - 2];
+                val += G_103 * pixels[local_y + 4][local_x - 1];
+                val += G_104 * pixels[local_y + 4][local_x    ];
+                val += G_105 * pixels[local_y + 4][local_x + 1];
+                val += G_106 * pixels[local_y + 4][local_x + 2];
+                val += G_107 * pixels[local_y + 4][local_x + 3];
+                val += G_108 * pixels[local_y + 4][local_x + 4];
+                val += G_109 * pixels[local_y + 4][local_x + 5];
+                val += G_110 * pixels[local_y + 5][local_x - 5];
+                val += G_111 * pixels[local_y + 5][local_x - 4];
+                val += G_112 * pixels[local_y + 5][local_x - 3];
+                val += G_113 * pixels[local_y + 5][local_x - 2];
+                val += G_114 * pixels[local_y + 5][local_x - 1];
+                val += G_115 * pixels[local_y + 5][local_x    ];
+                val += G_116 * pixels[local_y + 5][local_x + 1];
+                val += G_117 * pixels[local_y + 5][local_x + 2];
+                val += G_118 * pixels[local_y + 5][local_x + 3];
+                val += G_119 * pixels[local_y + 5][local_x + 4];
+                val += G_120 * pixels[local_y + 5][local_x + 5];
+            }
+
+            output[i * num_pix + pix_id] = val;
+        }
+        block.sync();
+    }
+}
+
+
+torch::Tensor conv2DForward(torch::Tensor &input) {
+    int H = input.size(1);
+    int W = input.size(2);
+	dim3 grid((W + BX - 1) / BX, (H + BY - 1) / BY, 1);
+	dim3 block(BX, BY, 1);
+
+    torch::Tensor aux = torch::zeros({3, H, W}, input.options());
+    convCUDA<3><<<grid, block>>>(
+		input.contiguous().data<float>(),
+		aux.contiguous().data<float>(),
+        H, W
+    );
+    return aux;
+
+
+    separableConvCUDA<3><<<grid, block>>>(
+		input.contiguous().data<float>(),
+		aux.contiguous().data<float>(),
+        H, W
+    );
+
+
+    // torch::Tensor aux_T = torch::full({3, W, H}, 0, input.options());
+	// grid = dim3((W + BLOCK_DIM - 1) / BLOCK_DIM, (H + BLOCK_DIM - 1) / BLOCK_DIM, 1);
+    // block = dim3(BLOCK_DIM, BLOCK_DIM, 1);
+    // transposeCUDA<3><<<grid, block>>>(
+    //     aux_T.contiguous().data<float>(),
+    //     aux.contiguous().data<float>(),
+    //     W,
+    //     H);
+
+    aux = aux.transpose(1,2);
+
+    std::swap(H, W);
+
+    torch::Tensor output_T = torch::full({3, H, W}, 0, input.options());
+	grid = dim3((W + BX - 1) / BX, (H + BY - 1) / BY, 1);
+    block = dim3(BX, BY, 1);
+    separableConvCUDA<3><<<grid, block>>>(
+		aux.contiguous().data<float>(),
+		output_T.contiguous().data<float>(),
+        H, W
+    );
+
+    // torch::Tensor output = torch::full({3, W, H}, 0, input.options());
+	// grid = dim3((W + BLOCK_DIM - 1) / BLOCK_DIM, (H + BLOCK_DIM - 1) / BLOCK_DIM, 1);
+    // block = dim3(BLOCK_DIM, BLOCK_DIM, 1);
+    // transposeCUDA<3><<<grid, block>>>(
+    //     output.contiguous().data<float>(),
+    //     output_T.contiguous().data<float>(),
+    //     W,
+    //     H);
+    // std::swap(H, W);
+    return output_T.transpose(1,2);
+}
+
+
+
+__global__ void ssimrestCUDA(int N, float C1, float C2, float* mu1, float* mu2, float* mim, float* mom, float* mu2_sq, float* sigma2_sq, float* ssim_map)
+{
+    int idx = threadIdx.x + blockDim.x * blockIdx.x;
+    if(idx >= N)
+        return;
+
+    float mu1_sq = mu1[idx] * mu1[idx];
+    float mu1_mu2 = mu1[idx] * mu2[idx];
+    float sigma1_sq = mim[idx] - mu1_sq;
+    float sigma12 = mom[idx] - mu1_mu2;
+
+    ssim_map[idx] = ((2.0f * mu1_mu2 + C1) * (2.0f * sigma12 + C2)) / ((mu1_sq + mu2_sq[idx] + C1) * (sigma1_sq + sigma2_sq[idx] + C2));
+}
+
+__global__ void ssimrest_backCUDA(
+    int N, float C1, float C2, float* mu1_, float* mu2_, float* mim_, float* mom_, float* mu2_sq_, float* sigma2_sq_, 
+    float* dL,
+    float* dL_dmu1,
+    float* dL_dmim,
+    float* dL_dmom)
+{
+    int idx = threadIdx.x + blockDim.x * blockIdx.x;
+    if(idx >= N)
+        return;
+
+    float mu1 = mu1_[idx];
+    float mu2 = mu2_[idx];
+    float mu2_sq = mu2_sq_[idx];
+    float mim = mim_[idx];
+    float mom = mom_[idx];
+    float sigma2_sq = sigma2_sq_[idx];
+
+    float A = (mu1*mu1 + C1 + mu2_sq);
+    float B = (- mu1*mu1 + C2 + mim + sigma2_sq);
+    float C = (C1 + 2*mu1*mu2);
+    float D = (C2 + 2*mom - 2*mu1*mu2);
+
+    float L = dL[idx];
+    dL_dmu1[idx] = L * ((2*mu2*D)/(A*B) - (2*mu2*C)/(A*B) + (2*mu1*C*D)/(A*B*B) - (2*mu1*C*D)/(A*A*B));
+    dL_dmim[idx] = L * (-(C*D)/(A*B*B));
+    dL_dmom[idx] = L * ((2*C)/(A*B));
+}
+
+__global__ void lol(float* hnk)
+{
+    hnk[0] = 42;
+}
+
+torch::Tensor ssimrest(
+	float C1, 
+	float C2, 
+	torch::Tensor& mu1, 
+	torch::Tensor& mu2, 
+	torch::Tensor& mim, 
+	torch::Tensor& mom, 
+	torch::Tensor& mu2_sq, 
+	torch::Tensor& sigma2_sq
+)
+{
+    int N = mu1.size(0) * mu1.size(1) * mu1.size(2);
+
+    torch::Tensor target = torch::zeros_like(mu1).contiguous();
+    ssimrestCUDA<<<(N + 255)/256, 256>>>(
+        N,
+        C1, 
+        C2, 
+        mu1.contiguous().data<float>(),
+        mu2.contiguous().data<float>(),
+        mim.contiguous().data<float>(),
+        mom.contiguous().data<float>(),        
+        mu2_sq.contiguous().data<float>(),
+        sigma2_sq.contiguous().data<float>(),
+        target.contiguous().data<float>());
+    return target;
+}
+
+std::tuple<torch::Tensor, torch::Tensor, torch::Tensor> ssimrest_back(
+	float C1, 
+	float C2, 
+	torch::Tensor& mu1, 
+	torch::Tensor& mu2, 
+	torch::Tensor& mim, 
+	torch::Tensor& mom, 
+	torch::Tensor& mu2_sq, 
+	torch::Tensor& sigma2_sq,
+    torch::Tensor& dL
+)
+{
+    int N = mu1.size(0) * mu1.size(1) * mu1.size(2);
+
+    torch::Tensor dL_dmu1 = torch::zeros_like(mu1).contiguous();
+    torch::Tensor dL_dmim = torch::zeros_like(mu1).contiguous();
+    torch::Tensor dL_dmom = torch::zeros_like(mu1).contiguous();
+    ssimrest_backCUDA<<<(N + 255)/256, 256>>>(
+        N,
+        C1, 
+        C2, 
+        mu1.contiguous().data<float>(),
+        mu2.contiguous().data<float>(),
+        mim.contiguous().data<float>(),
+        mom.contiguous().data<float>(),        
+        mu2_sq.contiguous().data<float>(),
+        sigma2_sq.contiguous().data<float>(),
+        dL.contiguous().data<float>(),
+        dL_dmu1.contiguous().data<float>(),
+        dL_dmim.contiguous().data<float>(),
+        dL_dmom.contiguous().data<float>());
+    return std::make_tuple(dL_dmu1, dL_dmim, dL_dmom);
+}
+
+template <int C>
+__device__ void load_into_shared(float pixels[BY + 10][BX + 10], float *input1, float *input2, int H, int W, int i, int subtract = 0) {
+	auto block = cg::this_thread_block();
+    const int start_y = block.group_index().y * (BY - subtract) - subtract / 2;
+    const int start_x = block.group_index().x * (BX - subtract) - subtract / 2;
+
+    const int cnt = (BY + 10) * (BX + 10);
+    const int num_blocks = (cnt + BX * BY - 1) / (BX * BY);
+    for (int b = 0; b < num_blocks; ++b) {
+        int tid = b * (BX * BY) + block.thread_rank();
+        if (tid < cnt) {
+            int local_y = tid / (BX + 10);
+            int local_x = tid % (BX + 10);
+            int y = start_y + local_y;
+            int x = start_x + local_x;
+            if (input2 == nullptr) {
+                float one = get_pix_value<C>(input1, i, y - 5, x - 5, H, W);
+                pixels[local_y][local_x] = one;
+            } else {
+                float one = get_pix_value<C>(input1, i, y - 5, x - 5, H, W);
+                float two = get_pix_value<C>(input2, i, y - 5, x - 5, H, W);
+                pixels[local_y][local_x] = one * two;
+            }
+        }
+    }
+}
+
+__device__ void write_to_shared(float pixels[BY + 10][BX + 10], float val) {
+	auto block = cg::this_thread_block();
+
+    // flush with 0s
+    const int cnt = (BY + 10) * (BX + 10);
+    const int num_blocks = (cnt + BX * BY - 1) / (BX * BY);
+    for (int b = 0; b < num_blocks; ++b) {
+        int tid = b * (BX * BY) + block.thread_rank();
+        if (tid < cnt) {
+            int local_y = tid / (BX + 10);
+            int local_x = tid % (BX + 10);
+            pixels[local_y][local_x] = 0.0f;
+        }
+    }
+    block.sync();
+
+    // write the values in the central BXxBY zone
+    pixels[block.thread_index().y + 5][block.thread_index().x + 5] = val;
+}
+
+__device__ void multiply_shared_mem(float pix1[BY + 10][BX + 10], float pix2[BY + 10][BX + 10]) {
+	auto block = cg::this_thread_block();
+    const int cnt = (BY + 10) * (BX + 10);
+    const int num_blocks = (cnt + BX * BY - 1) / (BX * BY);
+    for (int b = 0; b < num_blocks; ++b) {
+        int tid = b * (BX * BY) + block.thread_rank();
+        if (tid < cnt) {
+            int local_y = tid / (BX + 10);
+            int local_x = tid % (BX + 10);
+            float one = pix1[local_y][local_x];
+            float two = pix2[local_y][local_x];
+            pix1[local_y][local_x] = one * two;
+        }
+    }
+}
+
+__device__ inline float do_sq(float val) {
+    return val * val;
+}
+
+__device__ float do_conv(float pixels[BY + 10][BX + 10], int H, int W, bool sq = false) {
+	auto block = cg::this_thread_block();
+    int local_y = block.thread_index().y + 5;
+    int local_x = block.thread_index().x + 5;
+    float val = 0.0f;
+    
+        if (sq) {
+
+            val += G_000 * do_sq(pixels[local_y - 5][local_x - 5]);
+            val += G_001 * do_sq(pixels[local_y - 5][local_x - 4]);
+            val += G_002 * do_sq(pixels[local_y - 5][local_x - 3]);
+            val += G_003 * do_sq(pixels[local_y - 5][local_x - 2]);
+            val += G_004 * do_sq(pixels[local_y - 5][local_x - 1]);
+            val += G_005 * do_sq(pixels[local_y - 5][local_x    ]);
+            val += G_006 * do_sq(pixels[local_y - 5][local_x + 1]);
+            val += G_007 * do_sq(pixels[local_y - 5][local_x + 2]);
+            val += G_008 * do_sq(pixels[local_y - 5][local_x + 3]);
+            val += G_009 * do_sq(pixels[local_y - 5][local_x + 4]);
+            val += G_010 * do_sq(pixels[local_y - 5][local_x + 5]);
+            val += G_011 * do_sq(pixels[local_y - 4][local_x - 5]);
+            val += G_012 * do_sq(pixels[local_y - 4][local_x - 4]);
+            val += G_013 * do_sq(pixels[local_y - 4][local_x - 3]);
+            val += G_014 * do_sq(pixels[local_y - 4][local_x - 2]);
+            val += G_015 * do_sq(pixels[local_y - 4][local_x - 1]);
+            val += G_016 * do_sq(pixels[local_y - 4][local_x    ]);
+            val += G_017 * do_sq(pixels[local_y - 4][local_x + 1]);
+            val += G_018 * do_sq(pixels[local_y - 4][local_x + 2]);
+            val += G_019 * do_sq(pixels[local_y - 4][local_x + 3]);
+            val += G_020 * do_sq(pixels[local_y - 4][local_x + 4]);
+            val += G_021 * do_sq(pixels[local_y - 4][local_x + 5]);
+            val += G_022 * do_sq(pixels[local_y - 3][local_x - 5]);
+            val += G_023 * do_sq(pixels[local_y - 3][local_x - 4]);
+            val += G_024 * do_sq(pixels[local_y - 3][local_x - 3]);
+            val += G_025 * do_sq(pixels[local_y - 3][local_x - 2]);
+            val += G_026 * do_sq(pixels[local_y - 3][local_x - 1]);
+            val += G_027 * do_sq(pixels[local_y - 3][local_x    ]);
+            val += G_028 * do_sq(pixels[local_y - 3][local_x + 1]);
+            val += G_029 * do_sq(pixels[local_y - 3][local_x + 2]);
+            val += G_030 * do_sq(pixels[local_y - 3][local_x + 3]);
+            val += G_031 * do_sq(pixels[local_y - 3][local_x + 4]);
+            val += G_032 * do_sq(pixels[local_y - 3][local_x + 5]);
+            val += G_033 * do_sq(pixels[local_y - 2][local_x - 5]);
+            val += G_034 * do_sq(pixels[local_y - 2][local_x - 4]);
+            val += G_035 * do_sq(pixels[local_y - 2][local_x - 3]);
+            val += G_036 * do_sq(pixels[local_y - 2][local_x - 2]);
+            val += G_037 * do_sq(pixels[local_y - 2][local_x - 1]);
+            val += G_038 * do_sq(pixels[local_y - 2][local_x    ]);
+            val += G_039 * do_sq(pixels[local_y - 2][local_x + 1]);
+            val += G_040 * do_sq(pixels[local_y - 2][local_x + 2]);
+            val += G_041 * do_sq(pixels[local_y - 2][local_x + 3]);
+            val += G_042 * do_sq(pixels[local_y - 2][local_x + 4]);
+            val += G_043 * do_sq(pixels[local_y - 2][local_x + 5]);
+            val += G_044 * do_sq(pixels[local_y - 1][local_x - 5]);
+            val += G_045 * do_sq(pixels[local_y - 1][local_x - 4]);
+            val += G_046 * do_sq(pixels[local_y - 1][local_x - 3]);
+            val += G_047 * do_sq(pixels[local_y - 1][local_x - 2]);
+            val += G_048 * do_sq(pixels[local_y - 1][local_x - 1]);
+            val += G_049 * do_sq(pixels[local_y - 1][local_x    ]);
+            val += G_050 * do_sq(pixels[local_y - 1][local_x + 1]);
+            val += G_051 * do_sq(pixels[local_y - 1][local_x + 2]);
+            val += G_052 * do_sq(pixels[local_y - 1][local_x + 3]);
+            val += G_053 * do_sq(pixels[local_y - 1][local_x + 4]);
+            val += G_054 * do_sq(pixels[local_y - 1][local_x + 5]);
+            val += G_055 * do_sq(pixels[local_y    ][local_x - 5]);
+            val += G_056 * do_sq(pixels[local_y    ][local_x - 4]);
+            val += G_057 * do_sq(pixels[local_y    ][local_x - 3]);
+            val += G_058 * do_sq(pixels[local_y    ][local_x - 2]);
+            val += G_059 * do_sq(pixels[local_y    ][local_x - 1]);
+            val += G_060 * do_sq(pixels[local_y    ][local_x    ]);
+            val += G_061 * do_sq(pixels[local_y    ][local_x + 1]);
+            val += G_062 * do_sq(pixels[local_y    ][local_x + 2]);
+            val += G_063 * do_sq(pixels[local_y    ][local_x + 3]);
+            val += G_064 * do_sq(pixels[local_y    ][local_x + 4]);
+            val += G_065 * do_sq(pixels[local_y    ][local_x + 5]);
+            val += G_066 * do_sq(pixels[local_y + 1][local_x - 5]);
+            val += G_067 * do_sq(pixels[local_y + 1][local_x - 4]);
+            val += G_068 * do_sq(pixels[local_y + 1][local_x - 3]);
+            val += G_069 * do_sq(pixels[local_y + 1][local_x - 2]);
+            val += G_070 * do_sq(pixels[local_y + 1][local_x - 1]);
+            val += G_071 * do_sq(pixels[local_y + 1][local_x    ]);
+            val += G_072 * do_sq(pixels[local_y + 1][local_x + 1]);
+            val += G_073 * do_sq(pixels[local_y + 1][local_x + 2]);
+            val += G_074 * do_sq(pixels[local_y + 1][local_x + 3]);
+            val += G_075 * do_sq(pixels[local_y + 1][local_x + 4]);
+            val += G_076 * do_sq(pixels[local_y + 1][local_x + 5]);
+            val += G_077 * do_sq(pixels[local_y + 2][local_x - 5]);
+            val += G_078 * do_sq(pixels[local_y + 2][local_x - 4]);
+            val += G_079 * do_sq(pixels[local_y + 2][local_x - 3]);
+            val += G_080 * do_sq(pixels[local_y + 2][local_x - 2]);
+            val += G_081 * do_sq(pixels[local_y + 2][local_x - 1]);
+            val += G_082 * do_sq(pixels[local_y + 2][local_x    ]);
+            val += G_083 * do_sq(pixels[local_y + 2][local_x + 1]);
+            val += G_084 * do_sq(pixels[local_y + 2][local_x + 2]);
+            val += G_085 * do_sq(pixels[local_y + 2][local_x + 3]);
+            val += G_086 * do_sq(pixels[local_y + 2][local_x + 4]);
+            val += G_087 * do_sq(pixels[local_y + 2][local_x + 5]);
+            val += G_088 * do_sq(pixels[local_y + 3][local_x - 5]);
+            val += G_089 * do_sq(pixels[local_y + 3][local_x - 4]);
+            val += G_090 * do_sq(pixels[local_y + 3][local_x - 3]);
+            val += G_091 * do_sq(pixels[local_y + 3][local_x - 2]);
+            val += G_092 * do_sq(pixels[local_y + 3][local_x - 1]);
+            val += G_093 * do_sq(pixels[local_y + 3][local_x    ]);
+            val += G_094 * do_sq(pixels[local_y + 3][local_x + 1]);
+            val += G_095 * do_sq(pixels[local_y + 3][local_x + 2]);
+            val += G_096 * do_sq(pixels[local_y + 3][local_x + 3]);
+            val += G_097 * do_sq(pixels[local_y + 3][local_x + 4]);
+            val += G_098 * do_sq(pixels[local_y + 3][local_x + 5]);
+            val += G_099 * do_sq(pixels[local_y + 4][local_x - 5]);
+            val += G_100 * do_sq(pixels[local_y + 4][local_x - 4]);
+            val += G_101 * do_sq(pixels[local_y + 4][local_x - 3]);
+            val += G_102 * do_sq(pixels[local_y + 4][local_x - 2]);
+            val += G_103 * do_sq(pixels[local_y + 4][local_x - 1]);
+            val += G_104 * do_sq(pixels[local_y + 4][local_x    ]);
+            val += G_105 * do_sq(pixels[local_y + 4][local_x + 1]);
+            val += G_106 * do_sq(pixels[local_y + 4][local_x + 2]);
+            val += G_107 * do_sq(pixels[local_y + 4][local_x + 3]);
+            val += G_108 * do_sq(pixels[local_y + 4][local_x + 4]);
+            val += G_109 * do_sq(pixels[local_y + 4][local_x + 5]);
+            val += G_110 * do_sq(pixels[local_y + 5][local_x - 5]);
+            val += G_111 * do_sq(pixels[local_y + 5][local_x - 4]);
+            val += G_112 * do_sq(pixels[local_y + 5][local_x - 3]);
+            val += G_113 * do_sq(pixels[local_y + 5][local_x - 2]);
+            val += G_114 * do_sq(pixels[local_y + 5][local_x - 1]);
+            val += G_115 * do_sq(pixels[local_y + 5][local_x    ]);
+            val += G_116 * do_sq(pixels[local_y + 5][local_x + 1]);
+            val += G_117 * do_sq(pixels[local_y + 5][local_x + 2]);
+            val += G_118 * do_sq(pixels[local_y + 5][local_x + 3]);
+            val += G_119 * do_sq(pixels[local_y + 5][local_x + 4]);
+            val += G_120 * do_sq(pixels[local_y + 5][local_x + 5]);
+        } else {
+
+            val += G_000 * pixels[local_y - 5][local_x - 5];
+            val += G_001 * pixels[local_y - 5][local_x - 4];
+            val += G_002 * pixels[local_y - 5][local_x - 3];
+            val += G_003 * pixels[local_y - 5][local_x - 2];
+            val += G_004 * pixels[local_y - 5][local_x - 1];
+            val += G_005 * pixels[local_y - 5][local_x    ];
+            val += G_006 * pixels[local_y - 5][local_x + 1];
+            val += G_007 * pixels[local_y - 5][local_x + 2];
+            val += G_008 * pixels[local_y - 5][local_x + 3];
+            val += G_009 * pixels[local_y - 5][local_x + 4];
+            val += G_010 * pixels[local_y - 5][local_x + 5];
+            val += G_011 * pixels[local_y - 4][local_x - 5];
+            val += G_012 * pixels[local_y - 4][local_x - 4];
+            val += G_013 * pixels[local_y - 4][local_x - 3];
+            val += G_014 * pixels[local_y - 4][local_x - 2];
+            val += G_015 * pixels[local_y - 4][local_x - 1];
+            val += G_016 * pixels[local_y - 4][local_x    ];
+            val += G_017 * pixels[local_y - 4][local_x + 1];
+            val += G_018 * pixels[local_y - 4][local_x + 2];
+            val += G_019 * pixels[local_y - 4][local_x + 3];
+            val += G_020 * pixels[local_y - 4][local_x + 4];
+            val += G_021 * pixels[local_y - 4][local_x + 5];
+            val += G_022 * pixels[local_y - 3][local_x - 5];
+            val += G_023 * pixels[local_y - 3][local_x - 4];
+            val += G_024 * pixels[local_y - 3][local_x - 3];
+            val += G_025 * pixels[local_y - 3][local_x - 2];
+            val += G_026 * pixels[local_y - 3][local_x - 1];
+            val += G_027 * pixels[local_y - 3][local_x    ];
+            val += G_028 * pixels[local_y - 3][local_x + 1];
+            val += G_029 * pixels[local_y - 3][local_x + 2];
+            val += G_030 * pixels[local_y - 3][local_x + 3];
+            val += G_031 * pixels[local_y - 3][local_x + 4];
+            val += G_032 * pixels[local_y - 3][local_x + 5];
+            val += G_033 * pixels[local_y - 2][local_x - 5];
+            val += G_034 * pixels[local_y - 2][local_x - 4];
+            val += G_035 * pixels[local_y - 2][local_x - 3];
+            val += G_036 * pixels[local_y - 2][local_x - 2];
+            val += G_037 * pixels[local_y - 2][local_x - 1];
+            val += G_038 * pixels[local_y - 2][local_x    ];
+            val += G_039 * pixels[local_y - 2][local_x + 1];
+            val += G_040 * pixels[local_y - 2][local_x + 2];
+            val += G_041 * pixels[local_y - 2][local_x + 3];
+            val += G_042 * pixels[local_y - 2][local_x + 4];
+            val += G_043 * pixels[local_y - 2][local_x + 5];
+            val += G_044 * pixels[local_y - 1][local_x - 5];
+            val += G_045 * pixels[local_y - 1][local_x - 4];
+            val += G_046 * pixels[local_y - 1][local_x - 3];
+            val += G_047 * pixels[local_y - 1][local_x - 2];
+            val += G_048 * pixels[local_y - 1][local_x - 1];
+            val += G_049 * pixels[local_y - 1][local_x    ];
+            val += G_050 * pixels[local_y - 1][local_x + 1];
+            val += G_051 * pixels[local_y - 1][local_x + 2];
+            val += G_052 * pixels[local_y - 1][local_x + 3];
+            val += G_053 * pixels[local_y - 1][local_x + 4];
+            val += G_054 * pixels[local_y - 1][local_x + 5];
+            val += G_055 * pixels[local_y    ][local_x - 5];
+            val += G_056 * pixels[local_y    ][local_x - 4];
+            val += G_057 * pixels[local_y    ][local_x - 3];
+            val += G_058 * pixels[local_y    ][local_x - 2];
+            val += G_059 * pixels[local_y    ][local_x - 1];
+            val += G_060 * pixels[local_y    ][local_x    ];
+            val += G_061 * pixels[local_y    ][local_x + 1];
+            val += G_062 * pixels[local_y    ][local_x + 2];
+            val += G_063 * pixels[local_y    ][local_x + 3];
+            val += G_064 * pixels[local_y    ][local_x + 4];
+            val += G_065 * pixels[local_y    ][local_x + 5];
+            val += G_066 * pixels[local_y + 1][local_x - 5];
+            val += G_067 * pixels[local_y + 1][local_x - 4];
+            val += G_068 * pixels[local_y + 1][local_x - 3];
+            val += G_069 * pixels[local_y + 1][local_x - 2];
+            val += G_070 * pixels[local_y + 1][local_x - 1];
+            val += G_071 * pixels[local_y + 1][local_x    ];
+            val += G_072 * pixels[local_y + 1][local_x + 1];
+            val += G_073 * pixels[local_y + 1][local_x + 2];
+            val += G_074 * pixels[local_y + 1][local_x + 3];
+            val += G_075 * pixels[local_y + 1][local_x + 4];
+            val += G_076 * pixels[local_y + 1][local_x + 5];
+            val += G_077 * pixels[local_y + 2][local_x - 5];
+            val += G_078 * pixels[local_y + 2][local_x - 4];
+            val += G_079 * pixels[local_y + 2][local_x - 3];
+            val += G_080 * pixels[local_y + 2][local_x - 2];
+            val += G_081 * pixels[local_y + 2][local_x - 1];
+            val += G_082 * pixels[local_y + 2][local_x    ];
+            val += G_083 * pixels[local_y + 2][local_x + 1];
+            val += G_084 * pixels[local_y + 2][local_x + 2];
+            val += G_085 * pixels[local_y + 2][local_x + 3];
+            val += G_086 * pixels[local_y + 2][local_x + 4];
+            val += G_087 * pixels[local_y + 2][local_x + 5];
+            val += G_088 * pixels[local_y + 3][local_x - 5];
+            val += G_089 * pixels[local_y + 3][local_x - 4];
+            val += G_090 * pixels[local_y + 3][local_x - 3];
+            val += G_091 * pixels[local_y + 3][local_x - 2];
+            val += G_092 * pixels[local_y + 3][local_x - 1];
+            val += G_093 * pixels[local_y + 3][local_x    ];
+            val += G_094 * pixels[local_y + 3][local_x + 1];
+            val += G_095 * pixels[local_y + 3][local_x + 2];
+            val += G_096 * pixels[local_y + 3][local_x + 3];
+            val += G_097 * pixels[local_y + 3][local_x + 4];
+            val += G_098 * pixels[local_y + 3][local_x + 5];
+            val += G_099 * pixels[local_y + 4][local_x - 5];
+            val += G_100 * pixels[local_y + 4][local_x - 4];
+            val += G_101 * pixels[local_y + 4][local_x - 3];
+            val += G_102 * pixels[local_y + 4][local_x - 2];
+            val += G_103 * pixels[local_y + 4][local_x - 1];
+            val += G_104 * pixels[local_y + 4][local_x    ];
+            val += G_105 * pixels[local_y + 4][local_x + 1];
+            val += G_106 * pixels[local_y + 4][local_x + 2];
+            val += G_107 * pixels[local_y + 4][local_x + 3];
+            val += G_108 * pixels[local_y + 4][local_x + 4];
+            val += G_109 * pixels[local_y + 4][local_x + 5];
+            val += G_110 * pixels[local_y + 5][local_x - 5];
+            val += G_111 * pixels[local_y + 5][local_x - 4];
+            val += G_112 * pixels[local_y + 5][local_x - 3];
+            val += G_113 * pixels[local_y + 5][local_x - 2];
+            val += G_114 * pixels[local_y + 5][local_x - 1];
+            val += G_115 * pixels[local_y + 5][local_x    ];
+            val += G_116 * pixels[local_y + 5][local_x + 1];
+            val += G_117 * pixels[local_y + 5][local_x + 2];
+            val += G_118 * pixels[local_y + 5][local_x + 3];
+            val += G_119 * pixels[local_y + 5][local_x + 4];
+            val += G_120 * pixels[local_y + 5][local_x + 5];
+        }
+    return val;
+}
+
+template <int CH>
+__global__ void fusedssimCUDA(
+    int H,
+    int W,
+    float C1,
+    float C2,
+    float* img1,
+    float* img2,
+    float* ssim_map
+)
+{
+	auto block = cg::this_thread_block();
+    const int pix_y = block.group_index().y * BY + block.thread_index().y;
+    const int pix_x = block.group_index().x * BX + block.thread_index().x;
+    const int pix_id = pix_y * W + pix_x;
+    const int num_pix = H * W;
+
+    // stats for ssim
+    float mu1 = 0.0f;
+    float mu2 = 0.0f;
+    float sigma1_sq = 0.0f;
+    float sigma2_sq = 0.0f;
+    float sigma12 = 0.0f;
+
+    // shared memory that will be used to load pixels temporarily
+    __shared__ float buf1[BY + 10][BX + 10];
+    __shared__ float buf2[BY + 10][BX + 10];
+
+    // mu1 <- Conv(img1)
+    // sigma1_sq = Conv(img1 * img1) - mu1_sq
+    for (int i = 0; i < CH; ++i) {
+        // load into shared
+        load_into_shared<CH>(buf1, img1, nullptr, H, W, i);
+        block.sync();
+        // conv
+        mu1 = do_conv(buf1, H, W);
+        sigma1_sq = do_conv(buf1, H, W, true) - mu1 * mu1;
+        block.sync();
+
+    // mu2 <- Conv(img2)
+    // sigma2_sq = Conv(img2 * img2) - mu2_sq
+        // load into shared
+        load_into_shared<CH>(buf2, img2, nullptr, H, W, i);
+        block.sync();
+        // conv
+        mu2 = do_conv(buf2, H, W);
+        sigma2_sq = do_conv(buf2, H, W, true) - mu2 * mu2;
+        block.sync();
+
+    // sigma12 = Conv(img1 * img2) - mu1_mu2
+        // load into shared
+        multiply_shared_mem(buf1, buf2);
+        block.sync();
+        // conv
+        sigma12 = do_conv(buf1, H, W) - mu1 * mu2;
+        block.sync();
+
+        float mu1_sq = mu1 * mu1;
+        float mu2_sq = mu2 * mu2;
+        float mu1_mu2 = mu1 * mu2;
+        float C = (2.0f * mu1_mu2 + C1);
+        float D = (2.0f * sigma12 + C2);
+        float A = (mu1_sq + mu2_sq + C1);
+        float B = (sigma1_sq + sigma2_sq + C2);
+        float m = (C * D) / (A * B);
+        if (pix_x < W && pix_y < H) {
+            ssim_map[i * num_pix + pix_id] = m;
+        }
+    }
+}
+
+__device__ bool in_inner_window() {
+	auto block = cg::this_thread_block();
+    return 5 <= block.thread_index().y && block.thread_index().y < BY - 5 && 5 <= block.thread_index().x && block.thread_index().x < BX - 5;
+}
+
+template <int CH>
+__global__ void fusedssim_backwardCUDA(
+    int H,
+    int W,
+    float C1,
+    float C2,
+    float* img1,
+    float* img2,
+    float *dL_dmap,
+    float *dL_dimg1)
+{
+	auto block = cg::this_thread_block();
+    const int pix_y = block.group_index().y * (BY - 10) + block.thread_index().y - 5;
+    const int pix_x = block.group_index().x * (BX - 10) + block.thread_index().x - 5;
+    const int pix_id = pix_y * W + pix_x;
+    const int num_pix = H * W;
+
+    // stats for ssim
+    float mu1 = 0.0f;
+    float mu2 = 0.0f;
+    float sigma1_sq = 0.0f;
+    float sigma2_sq = 0.0f;
+    float sigma12 = 0.0f;
+
+    // shared memory that will be used to load pixels temporarily
+    __shared__ float buf1[BY + 10][BX + 10];
+    __shared__ float buf2[BY + 10][BX + 10];
+
+    // mu1 <- Conv(img1)
+    // sigma1_sq = Conv(img1 * img1) - mu1_sq
+    for (int i = 0; i < CH; ++i) {
+        // load into shared
+        load_into_shared<CH>(buf1, img1, nullptr, H, W, i, 10);
+        block.sync();
+        // conv
+        mu1 = do_conv(buf1, H, W);
+        sigma1_sq = do_conv(buf1, H, W, true) - mu1 * mu1;
+        block.sync();
+
+    // mu2 <- Conv(img2)
+    // sigma2_sq = Conv(img2 * img2) - mu2_sq
+        // load into shared
+        load_into_shared<CH>(buf2, img2, nullptr, H, W, i, 10);
+        block.sync();
+        // conv
+        mu2 = do_conv(buf2, H, W);
+        sigma2_sq = do_conv(buf2, H, W, true) - mu2 * mu2;
+        block.sync();
+
+    // sigma12 = Conv(img1 * img2) - mu1_mu2
+        // load into shared
+        multiply_shared_mem(buf2, buf1);
+        block.sync();
+        // conv
+        sigma12 = do_conv(buf2, H, W) - mu1 * mu2;
+        block.sync();
+
+        float mu1_sq = mu1 * mu1;
+        float mu2_sq = mu2 * mu2;
+        float mu1_mu2 = mu1 * mu2;
+        float C = (2.0f * mu1_mu2 + C1);
+        float D = (2.0f * sigma12 + C2);
+        float A = (mu1_sq + mu2_sq + C1);
+        float B = (sigma1_sq + sigma2_sq + C2);
+        float m = (C * D) / (A * B);
+        // if (in_inner_window() && pix_x < W && pix_y < H) {
+        //     ssim_map[i * num_pix + pix_id] = m;
+        //     MU1[i * num_pix + pix_id] = mu1;
+        //     MU2[i * num_pix + pix_id] = mu2;
+        //     SIGMA1_SQ[i * num_pix + pix_id] = sigma1_sq;
+        //     SIGMA2_SQ[i * num_pix + pix_id] = sigma2_sq;
+        //     SIGMA12[i * num_pix + pix_id] = sigma12;
+        // }
+
+        float dL_dm = 0.0f;
+        if (in_inner_window() && pix_x < W && pix_y < H)
+            dL_dm = dL_dmap[i * num_pix + pix_id];
+        float dL_dmu1 = dL_dm * (
+            (mu2 * 2.0f * D) / (A * B)
+            -(mu2 * 2.0f * C) / (A * B)
+            -(mu1 * 2.0f * C * D) / ( A * A * B)
+            +(mu1 * 2.0f * C * D) / (A * B * B)
+            );
+        float dL_dsigma1_sq = dL_dm * ((-C * D) / (A * B * B));
+        float dL_dsigma12 = dL_dm * ((2 * C) / (A * B));
+
+        float dL_dpix = 0.0f;
+        float tmp = 0.0f;
+
+        // gradient from mu1
+        write_to_shared(buf2, dL_dmu1);
+        block.sync();
+        tmp = do_conv(buf2, H, W);
+        block.sync();
+        dL_dpix += tmp;
+
+        // gradient from sigma1_sq
+        write_to_shared(buf2, dL_dsigma1_sq);
+        block.sync();
+        // tmp = get_pix_value<CH>(img1, i, pix_y, pix_x, H, W);
+        tmp = buf1[block.thread_index().y + 5][block.thread_index().x + 5];
+        tmp *= 2.0f * do_conv(buf2, H, W);
+        block.sync();
+        dL_dpix += tmp;
+        // write_to_shared(buf2, dL_dsigma1_sq * mu1);
+        // block.sync();
+        // tmp = -2.0f * do_conv(buf2, H, W);
+        // block.sync();
+        // dL_dpix += tmp;
+
+        // gradient from sigma12
+        write_to_shared(buf2, dL_dsigma12);
+        block.sync();
+        tmp = get_pix_value<CH>(img2, i, pix_y, pix_x, H, W);
+        tmp *= do_conv(buf2, H, W);
+        block.sync();
+        dL_dpix += tmp;
+        // write_to_shared(buf2, dL_dsigma12 * mu2);
+        // block.sync();
+        // tmp = - do_conv(buf2, H, W);
+        // block.sync();
+        // dL_dpix += tmp;
+
+        if (in_inner_window() && pix_x < W && pix_y < H)
+            dL_dimg1[i * num_pix + pix_id] = dL_dpix;
+    }
+}
+
+torch::Tensor
+fusedssim(
+    float C1,
+    float C2,
+    torch::Tensor &img1,
+    torch::Tensor &img2
+)
+{
+    int H = img1.size(1);
+    int W = img1.size(2);
+	dim3 grid((W + BX - 1) / BX, (H + BY - 1) / BY, 1);
+	dim3 block(BX, BY, 1);
+	// dim3 grid((W + (BX - 10) - 1) / (BX - 10), (H + (BY - 10) - 1) / (BY - 10), 1);
+	// dim3 block(BX, BY, 1);
+
+    torch::Tensor target = torch::zeros_like(img1).contiguous();
+    fusedssimCUDA<3><<<grid,block>>>(
+        H,
+        W,
+        C1,
+        C2,
+        img1.contiguous().data<float>(),
+        img2.contiguous().data<float>(),
+        target.contiguous().data<float>()
+    );
+
+    return target;
+}
+
+torch::Tensor
+fusedssim_backward(
+    float C1,
+    float C2,
+    torch::Tensor &img1,
+    torch::Tensor &img2,
+    torch::Tensor &dL_dmap
+)
+{
+    int H = img1.size(1);
+    int W = img1.size(2);
+	dim3 grid((W + (BX - 10) - 1) / (BX - 10), (H + (BY - 10) - 1) / (BY - 10), 1);
+	dim3 block(BX, BY, 1);
+
+    torch::Tensor dL_dimg1 = torch::zeros_like(img1).contiguous();
+
+    fusedssim_backwardCUDA<3><<<grid,block>>>(
+        H,
+        W,
+        C1,
+        C2,
+        img1.contiguous().data<float>(),
+        img2.contiguous().data<float>(),
+        dL_dmap.contiguous().data<float>(),
+        dL_dimg1.contiguous().data<float>()
+    );
+
+    return dL_dimg1;
+}
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.cu b/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.cu
new file mode 100644
index 0000000..cc7aa3a
--- /dev/null
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.cu
@@ -0,0 +1,67 @@
+#include "auxiliary.h"
+#include "adam.h"
+#include <cooperative_groups.h>
+#include <cooperative_groups/reduce.h>
+namespace cg = cooperative_groups;
+
+// step on a grid of size (N, M)
+// N is always number of gaussians
+__global__
+void adamUpdateCUDA(
+    float* __restrict__ param,
+    const float* __restrict__ param_grad,
+    float* __restrict__ exp_avg,
+    float* __restrict__ exp_avg_sq,
+    const bool* tiles_touched,
+    const float lr,
+    const float b1,
+    const float b2,
+    const float eps,
+    const uint32_t N,
+    const uint32_t M) {
+
+	auto p_idx = cg::this_grid().thread_rank();
+    const uint32_t g_idx = p_idx / M;
+    if (g_idx >= N) return;
+    if (tiles_touched[g_idx]) {
+        float Register_param_grad = param_grad[p_idx];
+        float Register_exp_avg = exp_avg[p_idx];
+        float Register_exp_avg_sq = exp_avg_sq[p_idx];
+        Register_exp_avg = b1 * Register_exp_avg + (1.0f - b1) * Register_param_grad;
+        Register_exp_avg_sq = b2 * Register_exp_avg_sq + (1.0f - b2) * Register_param_grad * Register_param_grad;
+        float step = -lr * Register_exp_avg / (sqrt(Register_exp_avg_sq) + eps);
+
+        param[p_idx] += step;
+        exp_avg[p_idx] = Register_exp_avg;
+        exp_avg_sq[p_idx] = Register_exp_avg_sq;
+    }
+}
+
+void ADAM::adamUpdate(
+    float* param,
+    const float* param_grad,
+    float* exp_avg,
+    float* exp_avg_sq,
+    const bool* tiles_touched,
+    const float lr,
+    const float b1,
+    const float b2,
+    const float eps,
+    const uint32_t N,
+    const uint32_t M) {
+
+    const uint32_t cnt = N * M;
+    adamUpdateCUDA<<<(cnt + 255) / 256, 256>>> (
+        param,
+        param_grad,
+        exp_avg,
+        exp_avg_sq,
+        tiles_touched,
+        lr,
+        b1,
+        b2,
+        eps,
+        N,
+        M
+    );
+}
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.h
new file mode 100644
index 0000000..648fb16
--- /dev/null
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/adam.h
@@ -0,0 +1,26 @@
+#ifndef CUDA_RASTERIZER_ADAM_H_INCLUDED
+#define CUDA_RASTERIZER_ADAM_H_INCLUDED
+
+#include <cuda.h>
+#include "cuda_runtime.h"
+#include "device_launch_parameters.h"
+#define GLM_FORCE_CUDA
+#include <glm/glm.hpp>
+
+namespace ADAM {
+
+void adamUpdate(
+    float* param,
+    const float* param_grad,
+    float* exp_avg,
+    float* exp_avg_sq,
+    const bool* tiles_touched,
+    const float lr,
+    const float b1,
+    const float b2,
+    const float eps,
+    const uint32_t N,
+    const uint32_t M);
+}
+
+#endif
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h
index 4d4b9b7..0e12b46 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/auxiliary.h
@@ -55,6 +55,18 @@ __forceinline__ __device__ void getRect(const float2 p, int max_radius, uint2& r
 	};
 }
 
+__forceinline__ __device__ void getRect(const float2 p, int2 ext_rect, uint2& rect_min, uint2& rect_max, dim3 grid)
+{
+	rect_min = {
+		min(grid.x, max((int)0, (int)((p.x - ext_rect.x) / BLOCK_X))),
+		min(grid.y, max((int)0, (int)((p.y - ext_rect.y) / BLOCK_Y)))
+	};
+	rect_max = {
+		min(grid.x, max((int)0, (int)((p.x + ext_rect.x + BLOCK_X - 1) / BLOCK_X))),
+		min(grid.y, max((int)0, (int)((p.y + ext_rect.y + BLOCK_Y - 1) / BLOCK_Y)))
+	};
+}
+
 __forceinline__ __device__ float3 transformPoint4x3(const float3& p, const float* matrix)
 {
 	float3 transformed = {
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu b/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu
index 4aa41e1..c014130 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.cu
@@ -17,13 +17,14 @@ namespace cg = cooperative_groups;
 
 // Backward pass for conversion of spherical harmonics to RGB for
 // each Gaussian.
-__device__ void computeColorFromSH(int idx, int deg, int max_coeffs, const glm::vec3* means, glm::vec3 campos, const float* shs, const bool* clamped, const glm::vec3* dL_dcolor, glm::vec3* dL_dmeans, glm::vec3* dL_dshs)
+__device__ void computeColorFromSH(int idx, int deg, int max_coeffs, const glm::vec3* means, glm::vec3 campos, const float* dc, const float* shs, const bool* clamped, const glm::vec3* dL_dcolor, glm::vec3* dL_dmeans, glm::vec3* dL_ddc, glm::vec3* dL_dshs)
 {
 	// Compute intermediate values, as it is done during forward
 	glm::vec3 pos = means[idx];
 	glm::vec3 dir_orig = pos - campos;
 	glm::vec3 dir = dir_orig / glm::length(dir_orig);
 
+	glm::vec3* direct_color = ((glm::vec3*)dc) + idx;
 	glm::vec3* sh = ((glm::vec3*)shs) + idx * max_coeffs;
 
 	// Use PyTorch rule for clamping: if clamping was applied,
@@ -41,23 +42,24 @@ __device__ void computeColorFromSH(int idx, int deg, int max_coeffs, const glm::
 	float z = dir.z;
 
 	// Target location for this Gaussian to write SH gradients to
+	glm::vec3* dL_ddirect_color = dL_ddc + idx;
 	glm::vec3* dL_dsh = dL_dshs + idx * max_coeffs;
 
 	// No tricks here, just high school-level calculus.
 	float dRGBdsh0 = SH_C0;
-	dL_dsh[0] = dRGBdsh0 * dL_dRGB;
+	dL_ddirect_color[0] = dRGBdsh0 * dL_dRGB;
 	if (deg > 0)
 	{
 		float dRGBdsh1 = -SH_C1 * y;
 		float dRGBdsh2 = SH_C1 * z;
 		float dRGBdsh3 = -SH_C1 * x;
-		dL_dsh[1] = dRGBdsh1 * dL_dRGB;
-		dL_dsh[2] = dRGBdsh2 * dL_dRGB;
-		dL_dsh[3] = dRGBdsh3 * dL_dRGB;
+		dL_dsh[0] = dRGBdsh1 * dL_dRGB;
+		dL_dsh[1] = dRGBdsh2 * dL_dRGB;
+		dL_dsh[2] = dRGBdsh3 * dL_dRGB;
 
-		dRGBdx = -SH_C1 * sh[3];
-		dRGBdy = -SH_C1 * sh[1];
-		dRGBdz = SH_C1 * sh[2];
+		dRGBdx = -SH_C1 * sh[2];
+		dRGBdy = -SH_C1 * sh[0];
+		dRGBdz = SH_C1 * sh[1];
 
 		if (deg > 1)
 		{
@@ -69,15 +71,15 @@ __device__ void computeColorFromSH(int idx, int deg, int max_coeffs, const glm::
 			float dRGBdsh6 = SH_C2[2] * (2.f * zz - xx - yy);
 			float dRGBdsh7 = SH_C2[3] * xz;
 			float dRGBdsh8 = SH_C2[4] * (xx - yy);
-			dL_dsh[4] = dRGBdsh4 * dL_dRGB;
-			dL_dsh[5] = dRGBdsh5 * dL_dRGB;
-			dL_dsh[6] = dRGBdsh6 * dL_dRGB;
-			dL_dsh[7] = dRGBdsh7 * dL_dRGB;
-			dL_dsh[8] = dRGBdsh8 * dL_dRGB;
+			dL_dsh[3] = dRGBdsh4 * dL_dRGB;
+			dL_dsh[4] = dRGBdsh5 * dL_dRGB;
+			dL_dsh[5] = dRGBdsh6 * dL_dRGB;
+			dL_dsh[6] = dRGBdsh7 * dL_dRGB;
+			dL_dsh[7] = dRGBdsh8 * dL_dRGB;
 
-			dRGBdx += SH_C2[0] * y * sh[4] + SH_C2[2] * 2.f * -x * sh[6] + SH_C2[3] * z * sh[7] + SH_C2[4] * 2.f * x * sh[8];
-			dRGBdy += SH_C2[0] * x * sh[4] + SH_C2[1] * z * sh[5] + SH_C2[2] * 2.f * -y * sh[6] + SH_C2[4] * 2.f * -y * sh[8];
-			dRGBdz += SH_C2[1] * y * sh[5] + SH_C2[2] * 2.f * 2.f * z * sh[6] + SH_C2[3] * x * sh[7];
+			dRGBdx += SH_C2[0] * y * sh[3] + SH_C2[2] * 2.f * -x * sh[5] + SH_C2[3] * z * sh[6] + SH_C2[4] * 2.f * x * sh[7];
+			dRGBdy += SH_C2[0] * x * sh[3] + SH_C2[1] * z * sh[4] + SH_C2[2] * 2.f * -y * sh[5] + SH_C2[4] * 2.f * -y * sh[7];
+			dRGBdz += SH_C2[1] * y * sh[4] + SH_C2[2] * 2.f * 2.f * z * sh[5] + SH_C2[3] * x * sh[6];
 
 			if (deg > 2)
 			{
@@ -88,38 +90,38 @@ __device__ void computeColorFromSH(int idx, int deg, int max_coeffs, const glm::
 				float dRGBdsh13 = SH_C3[4] * x * (4.f * zz - xx - yy);
 				float dRGBdsh14 = SH_C3[5] * z * (xx - yy);
 				float dRGBdsh15 = SH_C3[6] * x * (xx - 3.f * yy);
-				dL_dsh[9] = dRGBdsh9 * dL_dRGB;
-				dL_dsh[10] = dRGBdsh10 * dL_dRGB;
-				dL_dsh[11] = dRGBdsh11 * dL_dRGB;
-				dL_dsh[12] = dRGBdsh12 * dL_dRGB;
-				dL_dsh[13] = dRGBdsh13 * dL_dRGB;
-				dL_dsh[14] = dRGBdsh14 * dL_dRGB;
-				dL_dsh[15] = dRGBdsh15 * dL_dRGB;
+				dL_dsh[8] = dRGBdsh9 * dL_dRGB;
+				dL_dsh[9] = dRGBdsh10 * dL_dRGB;
+				dL_dsh[10] = dRGBdsh11 * dL_dRGB;
+				dL_dsh[11] = dRGBdsh12 * dL_dRGB;
+				dL_dsh[12] = dRGBdsh13 * dL_dRGB;
+				dL_dsh[13] = dRGBdsh14 * dL_dRGB;
+				dL_dsh[14] = dRGBdsh15 * dL_dRGB;
 
 				dRGBdx += (
-					SH_C3[0] * sh[9] * 3.f * 2.f * xy +
-					SH_C3[1] * sh[10] * yz +
-					SH_C3[2] * sh[11] * -2.f * xy +
-					SH_C3[3] * sh[12] * -3.f * 2.f * xz +
-					SH_C3[4] * sh[13] * (-3.f * xx + 4.f * zz - yy) +
-					SH_C3[5] * sh[14] * 2.f * xz +
-					SH_C3[6] * sh[15] * 3.f * (xx - yy));
+					SH_C3[0] * sh[8] * 3.f * 2.f * xy +
+					SH_C3[1] * sh[9] * yz +
+					SH_C3[2] * sh[10] * -2.f * xy +
+					SH_C3[3] * sh[11] * -3.f * 2.f * xz +
+					SH_C3[4] * sh[12] * (-3.f * xx + 4.f * zz - yy) +
+					SH_C3[5] * sh[13] * 2.f * xz +
+					SH_C3[6] * sh[14] * 3.f * (xx - yy));
 
 				dRGBdy += (
-					SH_C3[0] * sh[9] * 3.f * (xx - yy) +
-					SH_C3[1] * sh[10] * xz +
-					SH_C3[2] * sh[11] * (-3.f * yy + 4.f * zz - xx) +
-					SH_C3[3] * sh[12] * -3.f * 2.f * yz +
-					SH_C3[4] * sh[13] * -2.f * xy +
-					SH_C3[5] * sh[14] * -2.f * yz +
-					SH_C3[6] * sh[15] * -3.f * 2.f * xy);
+					SH_C3[0] * sh[8] * 3.f * (xx - yy) +
+					SH_C3[1] * sh[9] * xz +
+					SH_C3[2] * sh[10] * (-3.f * yy + 4.f * zz - xx) +
+					SH_C3[3] * sh[11] * -3.f * 2.f * yz +
+					SH_C3[4] * sh[12] * -2.f * xy +
+					SH_C3[5] * sh[13] * -2.f * yz +
+					SH_C3[6] * sh[14] * -3.f * 2.f * xy);
 
 				dRGBdz += (
-					SH_C3[1] * sh[10] * xy +
-					SH_C3[2] * sh[11] * 4.f * 2.f * yz +
-					SH_C3[3] * sh[12] * 3.f * (2.f * zz - xx - yy) +
-					SH_C3[4] * sh[13] * 4.f * 2.f * xz +
-					SH_C3[5] * sh[14] * (xx - yy));
+					SH_C3[1] * sh[9] * xy +
+					SH_C3[2] * sh[10] * 4.f * 2.f * yz +
+					SH_C3[3] * sh[11] * 3.f * (2.f * zz - xx - yy) +
+					SH_C3[4] * sh[12] * 4.f * 2.f * xz +
+					SH_C3[5] * sh[13] * (xx - yy));
 			}
 		}
 	}
@@ -348,6 +350,7 @@ __global__ void preprocessCUDA(
 	int P, int D, int M,
 	const float3* means,
 	const int* radii,
+	const float* dc,
 	const float* shs,
 	const bool* clamped,
 	const glm::vec3* scales,
@@ -359,6 +362,7 @@ __global__ void preprocessCUDA(
 	glm::vec3* dL_dmeans,
 	float* dL_dcolor,
 	float* dL_dcov3D,
+	float* dL_ddc,
 	float* dL_dsh,
 	glm::vec3* dL_dscale,
 	glm::vec4* dL_drot)
@@ -388,13 +392,199 @@ __global__ void preprocessCUDA(
 
 	// Compute gradient updates due to computing colors from SHs
 	if (shs)
-		computeColorFromSH(idx, D, M, (glm::vec3*)means, *campos, shs, clamped, (glm::vec3*)dL_dcolor, (glm::vec3*)dL_dmeans, (glm::vec3*)dL_dsh);
+		computeColorFromSH(idx, D, M, (glm::vec3*)means, *campos, dc, shs, clamped, (glm::vec3*)dL_dcolor, (glm::vec3*)dL_dmeans, (glm::vec3*)dL_ddc, (glm::vec3*)dL_dsh);
 
 	// Compute gradient updates due to computing covariance from scale/rotation
 	if (scales)
 		computeCov3D(idx, scales[idx], scale_modifier, rotations[idx], dL_dcov3D, dL_dscale, dL_drot);
 }
 
+template<uint32_t C>
+__global__ void
+PerGaussianRenderCUDA(
+	const uint2* __restrict__ ranges,
+	const uint32_t* __restrict__ point_list,
+	int W, int H, int B,
+	const uint32_t* __restrict__ per_tile_bucket_offset,
+	const uint32_t* __restrict__ bucket_to_tile,
+	const float* __restrict__ sampled_T, const float* __restrict__ sampled_ar,
+	const float* __restrict__ bg_color,
+	const float2* __restrict__ points_xy_image,
+	const float4* __restrict__ conic_opacity,
+	const float* __restrict__ colors,
+	const float* __restrict__ final_Ts,
+	const uint32_t* __restrict__ n_contrib,
+	const uint32_t* __restrict__ max_contrib,
+	const float* __restrict__ pixel_colors,
+	const float* __restrict__ dL_dpixels,
+	float3* __restrict__ dL_dmean2D,
+	float4* __restrict__ dL_dconic2D,
+	float* __restrict__ dL_dopacity,
+	float* __restrict__ dL_dcolors
+) {
+	// global_bucket_idx = warp_idx
+	auto block = cg::this_thread_block();
+	auto my_warp = cg::tiled_partition<32>(block);
+	uint32_t global_bucket_idx = block.group_index().x * my_warp.meta_group_size() + my_warp.meta_group_rank();
+	bool valid_bucket = global_bucket_idx < (uint32_t) B;
+	if (!valid_bucket) return;
+
+	bool valid_splat = false;
+
+	uint32_t tile_id, bbm;
+	uint2 range;
+	int num_splats_in_tile, bucket_idx_in_tile;
+	int splat_idx_in_tile, splat_idx_global;
+
+	tile_id = bucket_to_tile[global_bucket_idx];
+	range = ranges[tile_id];
+	num_splats_in_tile = range.y - range.x;
+	// What is the number of buckets before me? what is my offset?
+	bbm = tile_id == 0 ? 0 : per_tile_bucket_offset[tile_id - 1];
+	bucket_idx_in_tile = global_bucket_idx - bbm;
+	splat_idx_in_tile = bucket_idx_in_tile * 32 + my_warp.thread_rank();
+	splat_idx_global = range.x + splat_idx_in_tile;
+	valid_splat = (splat_idx_in_tile < num_splats_in_tile);
+
+	// if first gaussian in bucket is useless, then others are also useless
+	if (bucket_idx_in_tile * 32 >= max_contrib[tile_id]) {
+		return;
+	}
+
+	// Load Gaussian properties into registers
+	int gaussian_idx = 0;
+	float2 xy = {0.0f, 0.0f};
+	float4 con_o = {0.0f, 0.0f, 0.0f, 0.0f};
+	float c[C] = {0.0f};
+	if (valid_splat) {
+		gaussian_idx = point_list[splat_idx_global];
+		xy = points_xy_image[gaussian_idx];
+		con_o = conic_opacity[gaussian_idx];
+		for (int ch = 0; ch < C; ++ch)
+			c[ch] = colors[gaussian_idx * C + ch];
+	}
+
+	// Gradient accumulation variables
+	float Register_dL_dmean2D_x = 0.0f;
+	float Register_dL_dmean2D_y = 0.0f;
+	float Register_dL_dconic2D_x = 0.0f;
+	float Register_dL_dconic2D_y = 0.0f;
+	float Register_dL_dconic2D_w = 0.0f;
+	float Register_dL_dopacity = 0.0f;
+	float Register_dL_dcolors[C] = {0.0f};
+	
+	// tile metadata
+	const uint32_t horizontal_blocks = (W + BLOCK_X - 1) / BLOCK_X;
+	const uint2 tile = {tile_id % horizontal_blocks, tile_id / horizontal_blocks};
+	const uint2 pix_min = {tile.x * BLOCK_X, tile.y * BLOCK_Y};
+
+	// values useful for gradient calculation
+	float T;
+	float T_final;
+	float last_contributor;
+	float ar[C];
+	float dL_dpixel[C];
+	const float ddelx_dx = 0.5 * W;
+	const float ddely_dy = 0.5 * H;
+
+	// iterate over all pixels in the tile
+	for (int i = 0; i < BLOCK_SIZE + 31; ++i) {
+		// SHUFFLING
+
+		// At this point, T already has my (1 - alpha) multiplied.
+		// So pass this ready-made T value to next thread.
+		T = my_warp.shfl_up(T, 1);
+		last_contributor = my_warp.shfl_up(last_contributor, 1);
+		T_final = my_warp.shfl_up(T_final, 1);
+		for (int ch = 0; ch < C; ++ch) {
+			ar[ch] = my_warp.shfl_up(ar[ch], 1);
+			dL_dpixel[ch] = my_warp.shfl_up(dL_dpixel[ch], 1);
+		}
+
+		// which pixel index should this thread deal with?
+		int idx = i - my_warp.thread_rank();
+		const uint2 pix = {pix_min.x + idx % BLOCK_X, pix_min.y + idx / BLOCK_X};
+		const uint32_t pix_id = W * pix.y + pix.x;
+		const float2 pixf = {(float) pix.x, (float) pix.y};
+		bool valid_pixel = pix.x < W && pix.y < H;
+		
+		// every 32nd thread should read the stored state from memory
+		// TODO: perhaps store these things in shared memory?
+		if (valid_splat && valid_pixel && my_warp.thread_rank() == 0 && idx < BLOCK_SIZE) {
+			T = sampled_T[global_bucket_idx * BLOCK_SIZE + idx];
+			for (int ch = 0; ch < C; ++ch)
+				ar[ch] = -pixel_colors[ch * H * W + pix_id] + sampled_ar[global_bucket_idx * BLOCK_SIZE * C + ch * BLOCK_SIZE + idx];
+			T_final = final_Ts[pix_id];
+			last_contributor = n_contrib[pix_id];
+			for (int ch = 0; ch < C; ++ch) {
+				dL_dpixel[ch] = dL_dpixels[ch * H * W + pix_id];
+			}
+		}
+
+		// do work
+		if (valid_splat && valid_pixel && 0 <= idx && idx < BLOCK_SIZE) {
+			if (W <= pix.x || H <= pix.y) continue;
+
+			if (splat_idx_in_tile >= last_contributor) continue;
+
+			// compute blending values
+			const float2 d = { xy.x - pixf.x, xy.y - pixf.y };
+			const float power = -0.5f * (con_o.x * d.x * d.x + con_o.z * d.y * d.y) - con_o.y * d.x * d.y;
+			if (power > 0.0f) continue;
+			const float G = exp(power);
+			const float alpha = min(0.99f, con_o.w * G);
+			if (alpha < 1.0f / 255.0f) continue;
+			const float dchannel_dcolor = alpha * T;
+
+			// add the gradient contribution of this pixel to the gaussian
+			float bg_dot_dpixel = 0.0f;
+			float dL_dalpha = 0.0f;
+			for (int ch = 0; ch < C; ++ch) {
+				ar[ch] += T * alpha * c[ch];
+				const float &dL_dchannel = dL_dpixel[ch];
+				Register_dL_dcolors[ch] += dchannel_dcolor * dL_dchannel;
+				dL_dalpha += ((c[ch] * T) - (1.0f / (1.0f - alpha)) * (-ar[ch])) * dL_dchannel;
+
+				bg_dot_dpixel += bg_color[ch] * dL_dpixel[ch];
+			}
+			dL_dalpha += (-T_final / (1.0f - alpha)) * bg_dot_dpixel;
+			T *= (1.0f - alpha);
+
+
+			// Helpful reusable temporary variables
+			const float dL_dG = con_o.w * dL_dalpha;
+			const float gdx = G * d.x;
+			const float gdy = G * d.y;
+			const float dG_ddelx = -gdx * con_o.x - gdy * con_o.y;
+			const float dG_ddely = -gdy * con_o.z - gdx * con_o.y;
+
+			// accumulate the gradients
+			const float tmp_x = dL_dG * dG_ddelx * ddelx_dx;
+			Register_dL_dmean2D_x += tmp_x;
+			const float tmp_y = dL_dG * dG_ddely * ddely_dy;
+			Register_dL_dmean2D_y += tmp_y;
+
+			Register_dL_dconic2D_x += -0.5f * gdx * d.x * dL_dG;
+			Register_dL_dconic2D_y += -0.5f * gdx * d.y * dL_dG;
+			Register_dL_dconic2D_w += -0.5f * gdy * d.y * dL_dG;
+			Register_dL_dopacity += G * dL_dalpha;
+		}
+	}
+
+	// finally add the gradients using atomics
+	if (valid_splat) {
+		atomicAdd(&dL_dmean2D[gaussian_idx].x, Register_dL_dmean2D_x);
+		atomicAdd(&dL_dmean2D[gaussian_idx].y, Register_dL_dmean2D_y);
+		atomicAdd(&dL_dconic2D[gaussian_idx].x, Register_dL_dconic2D_x);
+		atomicAdd(&dL_dconic2D[gaussian_idx].y, Register_dL_dconic2D_y);
+		atomicAdd(&dL_dconic2D[gaussian_idx].w, Register_dL_dconic2D_w);
+		atomicAdd(&dL_dopacity[gaussian_idx], Register_dL_dopacity);
+		for (int ch = 0; ch < C; ++ch) {
+			atomicAdd(&dL_dcolors[gaussian_idx * C + ch], Register_dL_dcolors[ch]);
+		}
+	}
+}
+
 // Backward version of the rendering procedure.
 template <uint32_t C>
 __global__ void __launch_bounds__(BLOCK_X * BLOCK_Y)
@@ -541,6 +731,10 @@ renderCUDA(
 			const float dG_ddelx = -gdx * con_o.x - gdy * con_o.y;
 			const float dG_ddely = -gdy * con_o.z - gdx * con_o.y;
 
+			if(con_o.w*G > 0.99f){
+				continue;
+			}
+
 			// Update gradients w.r.t. 2D mean position of the Gaussian
 			atomicAdd(&dL_dmean2D[global_id].x, dL_dG * dG_ddelx * ddelx_dx);
 			atomicAdd(&dL_dmean2D[global_id].y, dL_dG * dG_ddely * ddely_dy);
@@ -560,6 +754,7 @@ void BACKWARD::preprocess(
 	int P, int D, int M,
 	const float3* means3D,
 	const int* radii,
+	const float* dc,
 	const float* shs,
 	const bool* clamped,
 	const glm::vec3* scales,
@@ -576,6 +771,7 @@ void BACKWARD::preprocess(
 	glm::vec3* dL_dmean3D,
 	float* dL_dcolor,
 	float* dL_dcov3D,
+	float* dL_ddc,
 	float* dL_dsh,
 	glm::vec3* dL_dscale,
 	glm::vec4* dL_drot)
@@ -601,10 +797,11 @@ void BACKWARD::preprocess(
 	// Propagate gradients for remaining steps: finish 3D mean gradients,
 	// propagate color gradients to SH (if desireD), propagate 3D covariance
 	// matrix gradients to scale and rotation.
-	preprocessCUDA<NUM_CHANNELS> << < (P + 255) / 256, 256 >> > (
+	preprocessCUDA<NUM_CHAFFELS> << < (P + 255) / 256, 256 >> > (
 		P, D, M,
 		(float3*)means3D,
 		radii,
+		dc,
 		shs,
 		clamped,
 		(glm::vec3*)scales,
@@ -616,38 +813,50 @@ void BACKWARD::preprocess(
 		(glm::vec3*)dL_dmean3D,
 		dL_dcolor,
 		dL_dcov3D,
+		dL_ddc,
 		dL_dsh,
 		dL_dscale,
 		dL_drot);
 }
 
 void BACKWARD::render(
-	const dim3 grid, const dim3 block,
+	const dim3 grid, dim3 block,
 	const uint2* ranges,
 	const uint32_t* point_list,
-	int W, int H,
+	int W, int H, int R, int B,
+	const uint32_t* per_bucket_tile_offset,
+	const uint32_t* bucket_to_tile,
+	const float* sampled_T, const float* sampled_ar,
 	const float* bg_color,
 	const float2* means2D,
 	const float4* conic_opacity,
 	const float* colors,
 	const float* final_Ts,
 	const uint32_t* n_contrib,
+	const uint32_t* max_contrib,
+	const float* pixel_colors,
 	const float* dL_dpixels,
 	float3* dL_dmean2D,
 	float4* dL_dconic2D,
 	float* dL_dopacity,
 	float* dL_dcolors)
 {
-	renderCUDA<NUM_CHANNELS> << <grid, block >> >(
+	const int THREADS = 32;
+	PerGaussianRenderCUDA<NUM_CHAFFELS> <<<((B*32) + THREADS - 1) / THREADS,THREADS>>>(
 		ranges,
 		point_list,
-		W, H,
+		W, H, B,
+		per_bucket_tile_offset,
+		bucket_to_tile,
+		sampled_T, sampled_ar,
 		bg_color,
 		means2D,
 		conic_opacity,
 		colors,
 		final_Ts,
 		n_contrib,
+		max_contrib,
+		pixel_colors,
 		dL_dpixels,
 		dL_dmean2D,
 		dL_dconic2D,
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.h
index 93dd2e4..5c816e4 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/backward.h
@@ -24,13 +24,18 @@ namespace BACKWARD
 		const dim3 grid, dim3 block,
 		const uint2* ranges,
 		const uint32_t* point_list,
-		int W, int H,
+		int W, int H, int R, int B,
+		const uint32_t* per_bucket_tile_offset,
+		const uint32_t* bucket_to_tile,
+		const float* sampled_T, const float* sampled_ar,
 		const float* bg_color,
 		const float2* means2D,
 		const float4* conic_opacity,
 		const float* colors,
 		const float* final_Ts,
 		const uint32_t* n_contrib,
+		const uint32_t* max_contrib,
+		const float* pixel_colors,
 		const float* dL_dpixels,
 		float3* dL_dmean2D,
 		float4* dL_dconic2D,
@@ -41,6 +46,7 @@ namespace BACKWARD
 		int P, int D, int M,
 		const float3* means,
 		const int* radii,
+		const float* dc,
 		const float* shs,
 		const bool* clamped,
 		const glm::vec3* scales,
@@ -57,6 +63,7 @@ namespace BACKWARD
 		glm::vec3* dL_dmeans,
 		float* dL_dcolor,
 		float* dL_dcov3D,
+		float* dL_ddc,
 		float* dL_dsh,
 		glm::vec3* dL_dscale,
 		glm::vec4* dL_drot);
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/config.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/config.h
index 2a912fb..e4ec258 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/config.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/config.h
@@ -12,7 +12,7 @@
 #ifndef CUDA_RASTERIZER_CONFIG_H_INCLUDED
 #define CUDA_RASTERIZER_CONFIG_H_INCLUDED
 
-#define NUM_CHANNELS 3 // Default 3, RGB
+#define NUM_CHAFFELS 3 // Default 3, RGB
 #define BLOCK_X 16
 #define BLOCK_Y 16
 
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu b/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu
index c419a32..7946e51 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.cu
@@ -11,13 +11,17 @@
 
 #include "forward.h"
 #include "auxiliary.h"
+#include <cuda.h>
+#include "cuda_runtime.h"
+#include "device_launch_parameters.h"
+#include <cub/cub.cuh>
 #include <cooperative_groups.h>
 #include <cooperative_groups/reduce.h>
 namespace cg = cooperative_groups;
 
 // Forward method for converting the input spherical harmonics
 // coefficients of each Gaussian to a simple RGB color.
-__device__ glm::vec3 computeColorFromSH(int idx, int deg, int max_coeffs, const glm::vec3* means, glm::vec3 campos, const float* shs, bool* clamped)
+__device__ glm::vec3 computeColorFromSH(int idx, int deg, int max_coeffs, const glm::vec3* means, glm::vec3 campos, const float* dc, const float* shs, bool* clamped)
 {
 	// The implementation is loosely based on code for 
 	// "Differentiable Point-Based Radiance Fields for 
@@ -26,37 +30,38 @@ __device__ glm::vec3 computeColorFromSH(int idx, int deg, int max_coeffs, const
 	glm::vec3 dir = pos - campos;
 	dir = dir / glm::length(dir);
 
+	glm::vec3* direct_color = ((glm::vec3*)dc) + idx;
 	glm::vec3* sh = ((glm::vec3*)shs) + idx * max_coeffs;
-	glm::vec3 result = SH_C0 * sh[0];
+	glm::vec3 result = SH_C0 * direct_color[0];
 
 	if (deg > 0)
 	{
 		float x = dir.x;
 		float y = dir.y;
 		float z = dir.z;
-		result = result - SH_C1 * y * sh[1] + SH_C1 * z * sh[2] - SH_C1 * x * sh[3];
+		result = result - SH_C1 * y * sh[0] + SH_C1 * z * sh[1] - SH_C1 * x * sh[2];
 
 		if (deg > 1)
 		{
 			float xx = x * x, yy = y * y, zz = z * z;
 			float xy = x * y, yz = y * z, xz = x * z;
 			result = result +
-				SH_C2[0] * xy * sh[4] +
-				SH_C2[1] * yz * sh[5] +
-				SH_C2[2] * (2.0f * zz - xx - yy) * sh[6] +
-				SH_C2[3] * xz * sh[7] +
-				SH_C2[4] * (xx - yy) * sh[8];
+				SH_C2[0] * xy * sh[3] +
+				SH_C2[1] * yz * sh[4] +
+				SH_C2[2] * (2.0f * zz - xx - yy) * sh[5] +
+				SH_C2[3] * xz * sh[6] +
+				SH_C2[4] * (xx - yy) * sh[7];
 
 			if (deg > 2)
 			{
 				result = result +
-					SH_C3[0] * y * (3.0f * xx - yy) * sh[9] +
-					SH_C3[1] * xy * z * sh[10] +
-					SH_C3[2] * y * (4.0f * zz - xx - yy) * sh[11] +
-					SH_C3[3] * z * (2.0f * zz - 3.0f * xx - 3.0f * yy) * sh[12] +
-					SH_C3[4] * x * (4.0f * zz - xx - yy) * sh[13] +
-					SH_C3[5] * z * (xx - yy) * sh[14] +
-					SH_C3[6] * x * (xx - 3.0f * yy) * sh[15];
+					SH_C3[0] * y * (3.0f * xx - yy) * sh[8] +
+					SH_C3[1] * xy * z * sh[9] +
+					SH_C3[2] * y * (4.0f * zz - xx - yy) * sh[10] +
+					SH_C3[3] * z * (2.0f * zz - 3.0f * xx - 3.0f * yy) * sh[11] +
+					SH_C3[4] * x * (4.0f * zz - xx - yy) * sh[12] +
+					SH_C3[5] * z * (xx - yy) * sh[13] +
+					SH_C3[6] * x * (xx - 3.0f * yy) * sh[14];
 			}
 		}
 	}
@@ -159,6 +164,7 @@ __global__ void preprocessCUDA(int P, int D, int M,
 	const float scale_modifier,
 	const glm::vec4* rotations,
 	const float* opacities,
+	const float* dc,
 	const float* shs,
 	bool* clamped,
 	const float* cov3D_precomp,
@@ -240,7 +246,7 @@ __global__ void preprocessCUDA(int P, int D, int M,
 	// spherical harmonics coefficients to RGB color.
 	if (colors_precomp == nullptr)
 	{
-		glm::vec3 result = computeColorFromSH(idx, D, M, (glm::vec3*)orig_points, *cam_pos, shs, clamped);
+		glm::vec3 result = computeColorFromSH(idx, D, M, (glm::vec3*)orig_points, *cam_pos, dc, shs, clamped);
 		rgb[idx * C + 0] = result.x;
 		rgb[idx * C + 1] = result.y;
 		rgb[idx * C + 2] = result.z;
@@ -250,6 +256,7 @@ __global__ void preprocessCUDA(int P, int D, int M,
 	depths[idx] = p_view.z;
 	radii[idx] = my_radius;
 	points_xy_image[idx] = point_image;
+
 	// Inverse 2D covariance and opacity neatly pack into one float4
 	conic_opacity[idx] = { conic.x, conic.y, conic.z, opacities[idx] };
 	tiles_touched[idx] = (rect_max.y - rect_min.y) * (rect_max.x - rect_min.x);
@@ -263,12 +270,15 @@ __global__ void __launch_bounds__(BLOCK_X * BLOCK_Y)
 renderCUDA(
 	const uint2* __restrict__ ranges,
 	const uint32_t* __restrict__ point_list,
+	const uint32_t* __restrict__ per_tile_bucket_offset, uint32_t* __restrict__ bucket_to_tile,
+	float* __restrict__ sampled_T, float* __restrict__ sampled_ar,
 	int W, int H,
 	const float2* __restrict__ points_xy_image,
 	const float* __restrict__ features,
 	const float4* __restrict__ conic_opacity,
 	float* __restrict__ final_T,
 	uint32_t* __restrict__ n_contrib,
+	uint32_t* __restrict__ max_contrib,
 	const float* __restrict__ bg_color,
 	float* __restrict__ out_color)
 {
@@ -287,10 +297,22 @@ renderCUDA(
 	bool done = !inside;
 
 	// Load start/end range of IDs to process in bit sorted list.
-	uint2 range = ranges[block.group_index().y * horizontal_blocks + block.group_index().x];
+	uint32_t tile_id = block.group_index().y * horizontal_blocks + block.group_index().x;
+	uint2 range = ranges[tile_id];
 	const int rounds = ((range.y - range.x + BLOCK_SIZE - 1) / BLOCK_SIZE);
 	int toDo = range.y - range.x;
 
+	// what is the number of buckets before me? what is my offset?
+	uint32_t bbm = tile_id == 0 ? 0 : per_tile_bucket_offset[tile_id - 1];
+	// let's first quickly also write the bucket-to-tile mapping
+	int num_buckets = (toDo + 31) / 32;
+	for (int i = 0; i < (num_buckets + BLOCK_SIZE - 1) / BLOCK_SIZE; ++i) {
+		int bucket_idx = i * BLOCK_SIZE + block.thread_rank();
+		if (bucket_idx < num_buckets) {
+			bucket_to_tile[bbm + bucket_idx] = tile_id;
+		}
+	}
+	
 	// Allocate storage for batches of collectively fetched data.
 	__shared__ int collected_id[BLOCK_SIZE];
 	__shared__ float2 collected_xy[BLOCK_SIZE];
@@ -324,6 +346,15 @@ renderCUDA(
 		// Iterate over current batch
 		for (int j = 0; !done && j < min(BLOCK_SIZE, toDo); j++)
 		{
+			// add incoming T value for every 32nd gaussian
+			if (j % 32 == 0) {
+				sampled_T[(bbm * BLOCK_SIZE) + block.thread_rank()] = T;
+				for (int ch = 0; ch < CHANNELS; ++ch) {
+					sampled_ar[(bbm * BLOCK_SIZE * CHANNELS) + ch * BLOCK_SIZE + block.thread_rank()] = C[ch];
+				}
+				++bbm;
+			}
+
 			// Keep track of current position in range
 			contributor++;
 
@@ -370,31 +401,47 @@ renderCUDA(
 		n_contrib[pix_id] = last_contributor;
 		for (int ch = 0; ch < CHANNELS; ch++)
 			out_color[ch * H * W + pix_id] = C[ch] + T * bg_color[ch];
+
+	}
+
+	// max reduce the last contributor
+    typedef cub::BlockReduce<uint32_t, BLOCK_SIZE> BlockReduce;
+    __shared__ typename BlockReduce::TempStorage temp_storage;
+    last_contributor = BlockReduce(temp_storage).Reduce(last_contributor, cub::Max());
+	if (block.thread_rank() == 0) {
+		max_contrib[tile_id] = last_contributor;
 	}
 }
 
+
 void FORWARD::render(
 	const dim3 grid, dim3 block,
 	const uint2* ranges,
 	const uint32_t* point_list,
+	const uint32_t* per_tile_bucket_offset, uint32_t* bucket_to_tile,
+	float* sampled_T, float* sampled_ar,
 	int W, int H,
 	const float2* means2D,
 	const float* colors,
 	const float4* conic_opacity,
 	float* final_T,
 	uint32_t* n_contrib,
+	uint32_t* max_contrib,
 	const float* bg_color,
 	float* out_color)
 {
-	renderCUDA<NUM_CHANNELS> << <grid, block >> > (
+	renderCUDA<NUM_CHAFFELS> << <grid, block >> > (
 		ranges,
 		point_list,
+		per_tile_bucket_offset, bucket_to_tile,
+		sampled_T, sampled_ar,
 		W, H,
 		means2D,
 		colors,
 		conic_opacity,
 		final_T,
 		n_contrib,
+		max_contrib,
 		bg_color,
 		out_color);
 }
@@ -405,6 +452,7 @@ void FORWARD::preprocess(int P, int D, int M,
 	const float scale_modifier,
 	const glm::vec4* rotations,
 	const float* opacities,
+	const float* dc,
 	const float* shs,
 	bool* clamped,
 	const float* cov3D_precomp,
@@ -425,13 +473,14 @@ void FORWARD::preprocess(int P, int D, int M,
 	uint32_t* tiles_touched,
 	bool prefiltered)
 {
-	preprocessCUDA<NUM_CHANNELS> << <(P + 255) / 256, 256 >> > (
+	preprocessCUDA<NUM_CHAFFELS> << <(P + 255) / 256, 256 >> > (
 		P, D, M,
 		means3D,
 		scales,
 		scale_modifier,
 		rotations,
 		opacities,
+		dc,
 		shs,
 		clamped,
 		cov3D_precomp,
@@ -450,6 +499,5 @@ void FORWARD::preprocess(int P, int D, int M,
 		conic_opacity,
 		grid,
 		tiles_touched,
-		prefiltered
-		);
+		prefiltered);
 }
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.h
index 3c11cb9..f6c0213 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/forward.h
@@ -17,6 +17,7 @@
 #include "device_launch_parameters.h"
 #define GLM_FORCE_CUDA
 #include <glm/glm.hpp>
+#include <functional>
 
 namespace FORWARD
 {
@@ -27,6 +28,7 @@ namespace FORWARD
 		const float scale_modifier,
 		const glm::vec4* rotations,
 		const float* opacities,
+		const float* dc,
 		const float* shs,
 		bool* clamped,
 		const float* cov3D_precomp,
@@ -52,12 +54,15 @@ namespace FORWARD
 		const dim3 grid, dim3 block,
 		const uint2* ranges,
 		const uint32_t* point_list,
+		const uint32_t* per_tile_bucket_offset, uint32_t* bucket_to_tile,
+		float* sampled_T, float* sampled_ar,
 		int W, int H,
 		const float2* points_xy_image,
 		const float* features,
 		const float4* conic_opacity,
 		float* final_T,
 		uint32_t* n_contrib,
+		uint32_t* max_contrib,
 		const float* bg_color,
 		float* out_color);
 }
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer.h
index 81544ef..8edbaa0 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer.h
@@ -28,14 +28,16 @@ namespace CudaRasterizer
 			float* projmatrix,
 			bool* present);
 
-		static int forward(
+		static std::tuple<int,int> forward(
 			std::function<char* (size_t)> geometryBuffer,
 			std::function<char* (size_t)> binningBuffer,
 			std::function<char* (size_t)> imageBuffer,
+			std::function<char* (size_t)> sampleBuffer,
 			const int P, int D, int M,
 			const float* background,
 			const int width, int height,
 			const float* means3D,
+			const float* dc,
 			const float* shs,
 			const float* colors_precomp,
 			const float* opacities,
@@ -53,10 +55,11 @@ namespace CudaRasterizer
 			bool debug = false);
 
 		static void backward(
-			const int P, int D, int M, int R,
+			const int P, int D, int M, int R, int B,
 			const float* background,
 			const int width, int height,
 			const float* means3D,
+			const float* dc,
 			const float* shs,
 			const float* colors_precomp,
 			const float* scales,
@@ -71,6 +74,7 @@ namespace CudaRasterizer
 			char* geom_buffer,
 			char* binning_buffer,
 			char* image_buffer,
+			char* sample_buffer,
 			const float* dL_dpix,
 			float* dL_dmean2D,
 			float* dL_dconic,
@@ -78,6 +82,7 @@ namespace CudaRasterizer
 			float* dL_dcolor,
 			float* dL_dmean3D,
 			float* dL_dcov3D,
+			float* dL_ddc,
 			float* dL_dsh,
 			float* dL_dscale,
 			float* dL_drot,
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu
index f8782ac..c61c4e8 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.cu
@@ -49,6 +49,56 @@ uint32_t getHigherMsb(uint32_t n)
 	return msb;
 }
 
+__device__ inline float evaluate_opacity_factor(const float dx, const float dy, const float4 co) {
+	return 0.5f * (co.x * dx * dx + co.z * dy * dy) + co.y * dx * dy;
+}
+
+template<uint32_t PATCH_WIDTH, uint32_t PATCH_HEIGHT>
+__device__ inline float max_contrib_power_rect_gaussian_float(
+	const float4 co, 
+	const float2 mean, 
+	const glm::vec2 rect_min,
+	const glm::vec2 rect_max,
+	glm::vec2& max_pos)
+{
+	const float x_min_diff = rect_min.x - mean.x;
+	const float x_left = x_min_diff > 0.0f;
+	// const float x_left = mean.x < rect_min.x;
+	const float not_in_x_range = x_left + (mean.x > rect_max.x);
+
+	const float y_min_diff = rect_min.y - mean.y;
+	const float y_above =  y_min_diff > 0.0f;
+	// const float y_above = mean.y < rect_min.y;
+	const float not_in_y_range = y_above + (mean.y > rect_max.y);
+
+	max_pos = {mean.x, mean.y};
+	float max_contrib_power = 0.0f;
+
+	if ((not_in_y_range + not_in_x_range) > 0.0f)
+	{
+		const float px = x_left * rect_min.x + (1.0f - x_left) * rect_max.x;
+		const float py = y_above * rect_min.y + (1.0f - y_above) * rect_max.y;
+
+		const float dx = copysign(float(PATCH_WIDTH), x_min_diff);
+		const float dy = copysign(float(PATCH_HEIGHT), y_min_diff);
+
+		const float diffx = mean.x - px;
+		const float diffy = mean.y - py;
+
+		const float rcp_dxdxcox = __frcp_rn(PATCH_WIDTH * PATCH_WIDTH * co.x); // = 1.0 / (dx*dx*co.x)
+		const float rcp_dydycoz = __frcp_rn(PATCH_HEIGHT * PATCH_HEIGHT * co.z); // = 1.0 / (dy*dy*co.z)
+
+		const float tx = not_in_y_range * __saturatef((dx * co.x * diffx + dx * co.y * diffy) * rcp_dxdxcox);
+		const float ty = not_in_x_range * __saturatef((dy * co.y * diffx + dy * co.z * diffy) * rcp_dydycoz);
+		max_pos = {px + tx * dx, py + ty * dy};
+		
+		const float2 max_pos_diff = {mean.x - max_pos.x, mean.y - max_pos.y};
+		max_contrib_power = evaluate_opacity_factor(max_pos_diff.x, max_pos_diff.y, co);
+	}
+
+	return max_contrib_power;
+}
+
 // Wrapper method to call auxiliary coarse frustum containment test.
 // Mark all Gaussians that pass it.
 __global__ void checkFrustum(int P,
@@ -70,12 +120,14 @@ __global__ void checkFrustum(int P,
 __global__ void duplicateWithKeys(
 	int P,
 	const float2* points_xy,
+	const float4* __restrict__ conic_opacity,
 	const float* depths,
 	const uint32_t* offsets,
 	uint64_t* gaussian_keys_unsorted,
 	uint32_t* gaussian_values_unsorted,
 	int* radii,
-	dim3 grid)
+	dim3 grid,
+	int2* rects)
 {
 	auto idx = cg::this_grid().thread_rank();
 	if (idx >= P)
@@ -86,9 +138,18 @@ __global__ void duplicateWithKeys(
 	{
 		// Find this Gaussian's offset in buffer for writing keys/values.
 		uint32_t off = (idx == 0) ? 0 : offsets[idx - 1];
+		const uint32_t offset_to = offsets[idx];
 		uint2 rect_min, rect_max;
 
-		getRect(points_xy[idx], radii[idx], rect_min, rect_max, grid);
+		if(rects == nullptr)
+			getRect(points_xy[idx], radii[idx], rect_min, rect_max, grid);
+		else
+			getRect(points_xy[idx], rects[idx], rect_min, rect_max, grid);
+
+		const float2 xy = points_xy[idx];
+		const float4 co = conic_opacity[idx];
+		const float opacity_threshold = 1.0f / 255.0f;
+		const float opacity_factor_threshold = logf(co.w / opacity_threshold);
 
 		// For each tile that the bounding rect overlaps, emit a 
 		// key/value pair. The key is |  tile ID  |      depth      |,
@@ -99,15 +160,33 @@ __global__ void duplicateWithKeys(
 		{
 			for (int x = rect_min.x; x < rect_max.x; x++)
 			{
+				const glm::vec2 tile_min(x * BLOCK_X, y * BLOCK_Y);
+				const glm::vec2 tile_max((x + 1) * BLOCK_X - 1, (y + 1) * BLOCK_Y - 1);
+
+				glm::vec2 max_pos;
+				float max_opac_factor = 0.0f;
+				max_opac_factor = max_contrib_power_rect_gaussian_float<BLOCK_X-1, BLOCK_Y-1>(co, xy, tile_min, tile_max, max_pos);
+				
 				uint64_t key = y * grid.x + x;
 				key <<= 32;
 				key |= *((uint32_t*)&depths[idx]);
+				if (max_opac_factor <= opacity_factor_threshold) {
 				gaussian_keys_unsorted[off] = key;
 				gaussian_values_unsorted[off] = idx;
 				off++;
 			}
 		}
 	}
+
+		for (; off < offset_to; ++off) {
+			uint64_t key = (uint32_t) -1;
+			key <<= 32;
+			const float depth = FLT_MAX;
+			key |= *((uint32_t*)&depth);
+			gaussian_values_unsorted[off] = static_cast<uint32_t>(-1);
+			gaussian_keys_unsorted[off] = key;
+		}
+	}
 }
 
 // Check keys to see if it is at the start/end of one tile's range in 
@@ -122,6 +201,8 @@ __global__ void identifyTileRanges(int L, uint64_t* point_list_keys, uint2* rang
 	// Read tile ID from key. Update start/end of tile range if at limit.
 	uint64_t key = point_list_keys[idx];
 	uint32_t currtile = key >> 32;
+	bool valid_tile = currtile != (uint32_t) -1;
+
 	if (idx == 0)
 		ranges[currtile].x = 0;
 	else
@@ -130,13 +211,26 @@ __global__ void identifyTileRanges(int L, uint64_t* point_list_keys, uint2* rang
 		if (currtile != prevtile)
 		{
 			ranges[prevtile].y = idx;
+			if (valid_tile) 
 			ranges[currtile].x = idx;
 		}
 	}
-	if (idx == L - 1)
+	if (idx == L - 1 && valid_tile)
 		ranges[currtile].y = L;
 }
 
+// for each tile, see how many buckets/warps are needed to store the state
+__global__ void perTileBucketCount(int T, uint2* ranges, uint32_t* bucketCount) {
+	auto idx = cg::this_grid().thread_rank();
+	if (idx >= T)
+		return;
+	
+	uint2 range = ranges[idx];
+	int num_splats = range.y - range.x;
+	int num_buckets = (num_splats + 31) / 32;
+	bucketCount[idx] = (uint32_t) num_buckets;
+}
+
 // Mark Gaussians as visible/invisible, based on view frustum testing
 void CudaRasterizer::Rasterizer::markVisible(
 	int P,
@@ -175,9 +269,29 @@ CudaRasterizer::ImageState CudaRasterizer::ImageState::fromChunk(char*& chunk, s
 	obtain(chunk, img.accum_alpha, N, 128);
 	obtain(chunk, img.n_contrib, N, 128);
 	obtain(chunk, img.ranges, N, 128);
+	int* dummy;
+	int* wummy;
+	cub::DeviceScan::InclusiveSum(nullptr, img.scan_size, dummy, wummy, N);
+	obtain(chunk, img.contrib_scan, img.scan_size, 128);
+
+	obtain(chunk, img.max_contrib, N, 128);
+	obtain(chunk, img.pixel_colors, N * NUM_CHAFFELS, 128);
+	obtain(chunk, img.bucket_count, N, 128);
+	obtain(chunk, img.bucket_offsets, N, 128);
+	cub::DeviceScan::InclusiveSum(nullptr, img.bucket_count_scan_size, img.bucket_count, img.bucket_count, N);
+	obtain(chunk, img.bucket_count_scanning_space, img.bucket_count_scan_size, 128);
+
 	return img;
 }
 
+CudaRasterizer::SampleState CudaRasterizer::SampleState::fromChunk(char *& chunk, size_t C) {
+	SampleState sample;
+	obtain(chunk, sample.bucket_to_tile, C * BLOCK_SIZE, 128);
+	obtain(chunk, sample.T, C * BLOCK_SIZE, 128);
+	obtain(chunk, sample.ar, NUM_CHAFFELS * C * BLOCK_SIZE, 128);
+	return sample;
+}
+
 CudaRasterizer::BinningState CudaRasterizer::BinningState::fromChunk(char*& chunk, size_t P)
 {
 	BinningState binning;
@@ -193,16 +307,38 @@ CudaRasterizer::BinningState CudaRasterizer::BinningState::fromChunk(char*& chun
 	return binning;
 }
 
+__global__ void zero(int N, int* space)
+{
+	int idx = threadIdx.x + blockDim.x * blockIdx.x;
+	if(idx >= N)
+		return;
+	space[idx] = 0;
+}
+
+__global__ void set(int N, uint32_t* where, int* space)
+{
+	int idx = threadIdx.x + blockDim.x * blockIdx.x;
+	if(idx >= N)
+		return;
+
+	int off = (idx == 0) ? 0 : where[idx-1];
+
+	space[off] = 1;
+}
+
+
 // Forward rendering procedure for differentiable rasterization
 // of Gaussians.
-int CudaRasterizer::Rasterizer::forward(
+std::tuple<int,int> CudaRasterizer::Rasterizer::forward(
 	std::function<char* (size_t)> geometryBuffer,
 	std::function<char* (size_t)> binningBuffer,
 	std::function<char* (size_t)> imageBuffer,
+	std::function<char* (size_t)> sampleBuffer,
 	const int P, int D, int M,
 	const float* background,
 	const int width, int height,
 	const float* means3D,
+	const float* dc,
 	const float* shs,
 	const float* colors_precomp,
 	const float* opacities,
@@ -239,7 +375,7 @@ int CudaRasterizer::Rasterizer::forward(
 	char* img_chunkptr = imageBuffer(img_chunk_size);
 	ImageState imgState = ImageState::fromChunk(img_chunkptr, width * height);
 
-	if (NUM_CHANNELS != 3 && colors_precomp == nullptr)
+	if (NUM_CHAFFELS != 3 && colors_precomp == nullptr)
 	{
 		throw std::runtime_error("For non-RGB, provide precomputed Gaussian colors!");
 	}
@@ -252,6 +388,7 @@ int CudaRasterizer::Rasterizer::forward(
 		scale_modifier,
 		(glm::vec4*)rotations,
 		opacities,
+		dc,
 		shs,
 		geomState.clamped,
 		cov3D_precomp,
@@ -289,12 +426,14 @@ int CudaRasterizer::Rasterizer::forward(
 	duplicateWithKeys << <(P + 255) / 256, 256 >> > (
 		P,
 		geomState.means2D,
+		geomState.conic_opacity,
 		geomState.depths,
 		geomState.point_offsets,
 		binningState.point_list_keys_unsorted,
 		binningState.point_list_unsorted,
 		radii,
-		tile_grid)
+		tile_grid,
+		nullptr)
 	CHECK_CUDA(, debug)
 
 	int bit = getHigherMsb(tile_grid.x * tile_grid.y);
@@ -317,31 +456,47 @@ int CudaRasterizer::Rasterizer::forward(
 			imgState.ranges);
 	CHECK_CUDA(, debug)
 
+ 	// bucket count
+	int num_tiles = tile_grid.x * tile_grid.y;
+	perTileBucketCount<<<(num_tiles + 255) / 256, 256>>>(num_tiles, imgState.ranges, imgState.bucket_count);
+	CHECK_CUDA(cub::DeviceScan::InclusiveSum(imgState.bucket_count_scanning_space, imgState.bucket_count_scan_size, imgState.bucket_count, imgState.bucket_offsets, num_tiles), debug)
+	unsigned int bucket_sum;
+	CHECK_CUDA(cudaMemcpy(&bucket_sum, imgState.bucket_offsets + num_tiles - 1, sizeof(unsigned int), cudaMemcpyDeviceToHost), debug);
+	// create a state to store. size is number is the total number of buckets * block_size
+	size_t sample_chunk_size = required<SampleState>(bucket_sum);
+	char* sample_chunkptr = sampleBuffer(sample_chunk_size);
+	SampleState sampleState = SampleState::fromChunk(sample_chunkptr, bucket_sum);
+
 	// Let each tile blend its range of Gaussians independently in parallel
 	const float* feature_ptr = colors_precomp != nullptr ? colors_precomp : geomState.rgb;
 	CHECK_CUDA(FORWARD::render(
 		tile_grid, block,
 		imgState.ranges,
 		binningState.point_list,
+		imgState.bucket_offsets, sampleState.bucket_to_tile,
+		sampleState.T, sampleState.ar,
 		width, height,
 		geomState.means2D,
 		feature_ptr,
 		geomState.conic_opacity,
 		imgState.accum_alpha,
 		imgState.n_contrib,
+		imgState.max_contrib,
 		background,
 		out_color), debug)
 
-	return num_rendered;
+	CHECK_CUDA(cudaMemcpy(imgState.pixel_colors, out_color, sizeof(float) * width * height * NUM_CHAFFELS, cudaMemcpyDeviceToDevice), debug);
+	return std::make_tuple(num_rendered, bucket_sum);
 }
 
 // Produce necessary gradients for optimization, corresponding
 // to forward render pass
 void CudaRasterizer::Rasterizer::backward(
-	const int P, int D, int M, int R,
+	const int P, int D, int M, int R, int B,
 	const float* background,
 	const int width, int height,
 	const float* means3D,
+	const float* dc,
 	const float* shs,
 	const float* colors_precomp,
 	const float* scales,
@@ -356,6 +511,7 @@ void CudaRasterizer::Rasterizer::backward(
 	char* geom_buffer,
 	char* binning_buffer,
 	char* img_buffer,
+	char* sample_buffer,
 	const float* dL_dpix,
 	float* dL_dmean2D,
 	float* dL_dconic,
@@ -363,6 +519,7 @@ void CudaRasterizer::Rasterizer::backward(
 	float* dL_dcolor,
 	float* dL_dmean3D,
 	float* dL_dcov3D,
+	float* dL_ddc,
 	float* dL_dsh,
 	float* dL_dscale,
 	float* dL_drot,
@@ -371,6 +528,7 @@ void CudaRasterizer::Rasterizer::backward(
 	GeometryState geomState = GeometryState::fromChunk(geom_buffer, P);
 	BinningState binningState = BinningState::fromChunk(binning_buffer, R);
 	ImageState imgState = ImageState::fromChunk(img_buffer, width * height);
+	SampleState sampleState = SampleState::fromChunk(sample_buffer, B);
 
 	if (radii == nullptr)
 	{
@@ -392,13 +550,19 @@ void CudaRasterizer::Rasterizer::backward(
 		block,
 		imgState.ranges,
 		binningState.point_list,
-		width, height,
+		width, height, R, B,
+		imgState.bucket_offsets,
+		sampleState.bucket_to_tile,
+		sampleState.T,
+		sampleState.ar,
 		background,
 		geomState.means2D,
 		geomState.conic_opacity,
 		color_ptr,
 		imgState.accum_alpha,
 		imgState.n_contrib,
+		imgState.max_contrib,
+		imgState.pixel_colors,
 		dL_dpix,
 		(float3*)dL_dmean2D,
 		(float4*)dL_dconic,
@@ -412,6 +576,7 @@ void CudaRasterizer::Rasterizer::backward(
 	CHECK_CUDA(BACKWARD::preprocess(P, D, M,
 		(float3*)means3D,
 		radii,
+		dc,
 		shs,
 		geomState.clamped,
 		(glm::vec3*)scales,
@@ -428,6 +593,7 @@ void CudaRasterizer::Rasterizer::backward(
 		(glm::vec3*)dL_dmean3D,
 		dL_dcolor,
 		dL_dcov3D,
+		dL_ddc,
 		dL_dsh,
 		(glm::vec3*)dL_dscale,
 		(glm::vec4*)dL_drot), debug)
diff --git a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.h b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.h
index bc3f0ec..2370948 100644
--- a/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.h
+++ b/submodules/diff-gaussian-rasterization/cuda_rasterizer/rasterizer_impl.h
@@ -45,25 +45,46 @@ namespace CudaRasterizer
 
 	struct ImageState
 	{
+		uint32_t *bucket_count;
+		uint32_t *bucket_offsets;
+		size_t bucket_count_scan_size;
+		char * bucket_count_scanning_space;
+		float* pixel_colors;
+		uint32_t* max_contrib;
+
+		size_t scan_size;
 		uint2* ranges;
 		uint32_t* n_contrib;
 		float* accum_alpha;
+		char* contrib_scan;
 
 		static ImageState fromChunk(char*& chunk, size_t N);
 	};
 
 	struct BinningState
 	{
+		size_t scan_size;
 		size_t sorting_size;
 		uint64_t* point_list_keys_unsorted;
 		uint64_t* point_list_keys;
 		uint32_t* point_list_unsorted;
 		uint32_t* point_list;
+		int* scan_src;
+		int* scan_dst;
+		char* scan_space;
 		char* list_sorting_space;
 
 		static BinningState fromChunk(char*& chunk, size_t P);
 	};
 
+	struct SampleState
+	{
+		uint32_t *bucket_to_tile;
+		float *T;
+		float *ar;
+		static SampleState fromChunk(char*& chunk, size_t C);
+	};
+
 	template<typename T> 
 	size_t required(size_t P)
 	{
diff --git a/submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py b/submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py
index bbef37d..51551a7 100644
--- a/submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py
+++ b/submodules/diff-gaussian-rasterization/diff_gaussian_rasterization/__init__.py
@@ -21,6 +21,7 @@ def cpu_deep_copy_tuple(input_tuple):
 def rasterize_gaussians(
     means3D,
     means2D,
+    dc,
     sh,
     colors_precomp,
     opacities,
@@ -32,6 +33,7 @@ def rasterize_gaussians(
     return _RasterizeGaussians.apply(
         means3D,
         means2D,
+        dc,
         sh,
         colors_precomp,
         opacities,
@@ -47,16 +49,18 @@ class _RasterizeGaussians(torch.autograd.Function):
         ctx,
         means3D,
         means2D,
+        dc,
         sh,
         colors_precomp,
         opacities,
         scales,
         rotations,
         cov3Ds_precomp,
-        raster_settings,
+        raster_settings
     ):
 
         # Restructure arguments the way that the C++ lib expects them
+        
         args = (
             raster_settings.bg, 
             means3D,
@@ -72,6 +76,7 @@ class _RasterizeGaussians(torch.autograd.Function):
             raster_settings.tanfovy,
             raster_settings.image_height,
             raster_settings.image_width,
+            dc,
             sh,
             raster_settings.sh_degree,
             raster_settings.campos,
@@ -83,27 +88,29 @@ class _RasterizeGaussians(torch.autograd.Function):
         if raster_settings.debug:
             cpu_args = cpu_deep_copy_tuple(args) # Copy them before they can be corrupted
             try:
-                num_rendered, color, radii, geomBuffer, binningBuffer, imgBuffer = _C.rasterize_gaussians(*args)
+                num_rendered, num_buckets, color, radii, geomBuffer, binningBuffer, imgBuffer, sampleBuffer = _C.rasterize_gaussians(*args)
             except Exception as ex:
                 torch.save(cpu_args, "snapshot_fw.dump")
                 print("\nAn error occured in forward. Please forward snapshot_fw.dump for debugging.")
                 raise ex
         else:
-            num_rendered, color, radii, geomBuffer, binningBuffer, imgBuffer = _C.rasterize_gaussians(*args)
+            num_rendered, num_buckets, color, radii, geomBuffer, binningBuffer, imgBuffer, sampleBuffer = _C.rasterize_gaussians(*args)
 
         # Keep relevant tensors for backward
         ctx.raster_settings = raster_settings
         ctx.num_rendered = num_rendered
-        ctx.save_for_backward(colors_precomp, means3D, scales, rotations, cov3Ds_precomp, radii, sh, geomBuffer, binningBuffer, imgBuffer)
+        ctx.num_buckets = num_buckets
+        ctx.save_for_backward(colors_precomp, means3D, scales, rotations, cov3Ds_precomp, radii, dc, sh, geomBuffer, binningBuffer, imgBuffer, sampleBuffer)
         return color, radii
 
     @staticmethod
-    def backward(ctx, grad_out_color, _):
+    def backward(ctx, grad_out_color, *_):
 
         # Restore necessary values from context
         num_rendered = ctx.num_rendered
+        num_buckets = ctx.num_buckets
         raster_settings = ctx.raster_settings
-        colors_precomp, means3D, scales, rotations, cov3Ds_precomp, radii, sh, geomBuffer, binningBuffer, imgBuffer = ctx.saved_tensors
+        colors_precomp, means3D, scales, rotations, cov3Ds_precomp, radii, dc, sh, geomBuffer, binningBuffer, imgBuffer, sampleBuffer = ctx.saved_tensors
 
         # Restructure args as C++ method expects them
         args = (raster_settings.bg,
@@ -119,6 +126,7 @@ class _RasterizeGaussians(torch.autograd.Function):
                 raster_settings.tanfovx, 
                 raster_settings.tanfovy, 
                 grad_out_color, 
+                dc,
                 sh, 
                 raster_settings.sh_degree, 
                 raster_settings.campos,
@@ -126,23 +134,26 @@ class _RasterizeGaussians(torch.autograd.Function):
                 num_rendered,
                 binningBuffer,
                 imgBuffer,
+                num_buckets,
+                sampleBuffer,
                 raster_settings.debug)
 
         # Compute gradients for relevant tensors by invoking backward method
         if raster_settings.debug:
             cpu_args = cpu_deep_copy_tuple(args) # Copy them before they can be corrupted
             try:
-                grad_means2D, grad_colors_precomp, grad_opacities, grad_means3D, grad_cov3Ds_precomp, grad_sh, grad_scales, grad_rotations = _C.rasterize_gaussians_backward(*args)
+                grad_means2D, grad_colors_precomp, grad_opacities, grad_means3D, grad_cov3Ds_precomp, grad_dc, grad_sh, grad_scales, grad_rotations = _C.rasterize_gaussians_backward(*args)
             except Exception as ex:
                 torch.save(cpu_args, "snapshot_bw.dump")
                 print("\nAn error occured in backward. Writing snapshot_bw.dump for debugging.\n")
                 raise ex
         else:
-             grad_means2D, grad_colors_precomp, grad_opacities, grad_means3D, grad_cov3Ds_precomp, grad_sh, grad_scales, grad_rotations = _C.rasterize_gaussians_backward(*args)
+             grad_means2D, grad_colors_precomp, grad_opacities, grad_means3D, grad_cov3Ds_precomp, grad_dc, grad_sh, grad_scales, grad_rotations = _C.rasterize_gaussians_backward(*args)
 
         grads = (
             grad_means3D,
             grad_means2D,
+            grad_dc,
             grad_sh,
             grad_colors_precomp,
             grad_opacities,
@@ -184,7 +195,7 @@ class GaussianRasterizer(nn.Module):
             
         return visible
 
-    def forward(self, means3D, means2D, opacities, shs = None, colors_precomp = None, scales = None, rotations = None, cov3D_precomp = None):
+    def forward(self, means3D, means2D, opacities, dc = None, shs = None, colors_precomp = None, scales = None, rotations = None, cov3D_precomp = None):
         
         raster_settings = self.raster_settings
 
@@ -194,6 +205,8 @@ class GaussianRasterizer(nn.Module):
         if ((scales is None or rotations is None) and cov3D_precomp is None) or ((scales is not None or rotations is not None) and cov3D_precomp is not None):
             raise Exception('Please provide exactly one of either scale/rotation pair or precomputed 3D covariance!')
         
+        if dc is None:
+            dc = torch.Tensor([])
         if shs is None:
             shs = torch.Tensor([])
         if colors_precomp is None:
@@ -210,12 +223,41 @@ class GaussianRasterizer(nn.Module):
         return rasterize_gaussians(
             means3D,
             means2D,
+            dc,
             shs,
             colors_precomp,
             opacities,
             scales, 
             rotations,
             cov3D_precomp,
-            raster_settings, 
+            raster_settings
         )
 
+class SparseGaussianAdam(torch.optim.Adam):
+    def __init__(self, params, lr, eps):
+        super().__init__(params=params, lr=lr, eps=eps)
+    
+    @torch.no_grad()
+    def step(self, visibility, N):
+        for group in self.param_groups:
+            lr = group["lr"]
+            eps = group["eps"]
+
+            assert len(group["params"]) == 1, "more than one tensor in group"
+            param = group["params"][0]
+            if param.grad is None:
+                continue
+
+            # Lazy state initialization
+            state = self.state[param]
+            if len(state) == 0:
+                state['step'] = torch.tensor(0.0, dtype=torch.float32)
+                state['exp_avg'] = torch.zeros_like(param, memory_format=torch.preserve_format)
+                state['exp_avg_sq'] = torch.zeros_like(param, memory_format=torch.preserve_format)
+
+
+            stored_state = self.state.get(param, None)
+            exp_avg = stored_state["exp_avg"]
+            exp_avg_sq = stored_state["exp_avg_sq"]
+            M = param.numel() // N
+            _C.adamUpdate(param, param.grad, exp_avg, exp_avg_sq, visibility, lr, 0.9, 0.999, eps, N, M)
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/ext.cpp b/submodules/diff-gaussian-rasterization/ext.cpp
index d768779..4d00b5e 100644
--- a/submodules/diff-gaussian-rasterization/ext.cpp
+++ b/submodules/diff-gaussian-rasterization/ext.cpp
@@ -16,4 +16,7 @@ PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
   m.def("rasterize_gaussians", &RasterizeGaussiansCUDA);
   m.def("rasterize_gaussians_backward", &RasterizeGaussiansBackwardCUDA);
   m.def("mark_visible", &markVisible);
+  m.def("adamUpdate", &adamUpdate);
+  m.def("fusedssim", &fusedssim);
+  m.def("fusedssim_backward", &fusedssim_backward);
 }
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/rasterize_points.cu b/submodules/diff-gaussian-rasterization/rasterize_points.cu
index ddc5cf8..8a98f51 100644
--- a/submodules/diff-gaussian-rasterization/rasterize_points.cu
+++ b/submodules/diff-gaussian-rasterization/rasterize_points.cu
@@ -20,6 +20,7 @@
 #include <memory>
 #include "cuda_rasterizer/config.h"
 #include "cuda_rasterizer/rasterizer.h"
+#include "cuda_rasterizer/adam.h"
 #include <fstream>
 #include <string>
 #include <functional>
@@ -32,7 +33,23 @@ std::function<char*(size_t N)> resizeFunctional(torch::Tensor& t) {
     return lambda;
 }
 
-std::tuple<int, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
+std::function<int*(size_t N)> resizeIntFunctional(torch::Tensor& t) {
+    auto lambda = [&t](size_t N) {
+        t.resize_({(long long)N});
+		return t.contiguous().data_ptr<int>();
+    };
+    return lambda;
+}
+
+std::function<float*(size_t N)> resizeFloatFunctional(torch::Tensor& t) {
+    auto lambda = [&t](size_t N) {
+        t.resize_({(long long)N});
+		return t.contiguous().data_ptr<float>();
+    };
+    return lambda;
+}
+
+std::tuple<int, int, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
 RasterizeGaussiansCUDA(
 	const torch::Tensor& background,
 	const torch::Tensor& means3D,
@@ -48,6 +65,7 @@ RasterizeGaussiansCUDA(
 	const float tan_fovy,
     const int image_height,
     const int image_width,
+	const torch::Tensor& dc,
 	const torch::Tensor& sh,
 	const int degree,
 	const torch::Tensor& campos,
@@ -65,7 +83,7 @@ RasterizeGaussiansCUDA(
   auto int_opts = means3D.options().dtype(torch::kInt32);
   auto float_opts = means3D.options().dtype(torch::kFloat32);
 
-  torch::Tensor out_color = torch::full({NUM_CHANNELS, H, W}, 0.0, float_opts);
+  torch::Tensor out_color = torch::full({NUM_CHAFFELS, H, W}, 0.0, float_opts);
   torch::Tensor radii = torch::full({P}, 0, means3D.options().dtype(torch::kInt32));
   
   torch::Device device(torch::kCUDA);
@@ -73,11 +91,14 @@ RasterizeGaussiansCUDA(
   torch::Tensor geomBuffer = torch::empty({0}, options.device(device));
   torch::Tensor binningBuffer = torch::empty({0}, options.device(device));
   torch::Tensor imgBuffer = torch::empty({0}, options.device(device));
+  torch::Tensor sampleBuffer = torch::empty({0}, options.device(device));
   std::function<char*(size_t)> geomFunc = resizeFunctional(geomBuffer);
   std::function<char*(size_t)> binningFunc = resizeFunctional(binningBuffer);
   std::function<char*(size_t)> imgFunc = resizeFunctional(imgBuffer);
+  std::function<char*(size_t)> sampleFunc = resizeFunctional(sampleBuffer);
   
   int rendered = 0;
+  int num_buckets = 0;
   if(P != 0)
   {
 	  int M = 0;
@@ -86,14 +107,16 @@ RasterizeGaussiansCUDA(
 		M = sh.size(1);
       }
 
-	  rendered = CudaRasterizer::Rasterizer::forward(
+	  auto tup = CudaRasterizer::Rasterizer::forward(
 	    geomFunc,
 		binningFunc,
 		imgFunc,
+		sampleFunc,
 	    P, degree, M,
 		background.contiguous().data<float>(),
 		W, H,
 		means3D.contiguous().data<float>(),
+		dc.contiguous().data_ptr<float>(),
 		sh.contiguous().data_ptr<float>(),
 		colors.contiguous().data<float>(), 
 		opacity.contiguous().data<float>(), 
@@ -110,11 +133,14 @@ RasterizeGaussiansCUDA(
 		out_color.contiguous().data<float>(),
 		radii.contiguous().data<int>(),
 		debug);
+		
+		rendered = std::get<0>(tup);
+		num_buckets = std::get<1>(tup);
   }
-  return std::make_tuple(rendered, out_color, radii, geomBuffer, binningBuffer, imgBuffer);
+  return std::make_tuple(rendered, num_buckets, out_color, radii, geomBuffer, binningBuffer, imgBuffer, sampleBuffer);
 }
 
-std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
+std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
  RasterizeGaussiansBackwardCUDA(
  	const torch::Tensor& background,
 	const torch::Tensor& means3D,
@@ -129,6 +155,7 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	const float tan_fovx,
 	const float tan_fovy,
     const torch::Tensor& dL_dout_color,
+	const torch::Tensor& dc,
 	const torch::Tensor& sh,
 	const int degree,
 	const torch::Tensor& campos,
@@ -136,6 +163,8 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	const int R,
 	const torch::Tensor& binningBuffer,
 	const torch::Tensor& imageBuffer,
+	const int B,
+	const torch::Tensor& sampleBuffer,
 	const bool debug) 
 {
   const int P = means3D.size(0);
@@ -150,20 +179,22 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 
   torch::Tensor dL_dmeans3D = torch::zeros({P, 3}, means3D.options());
   torch::Tensor dL_dmeans2D = torch::zeros({P, 3}, means3D.options());
-  torch::Tensor dL_dcolors = torch::zeros({P, NUM_CHANNELS}, means3D.options());
+  torch::Tensor dL_dcolors = torch::zeros({P, NUM_CHAFFELS}, means3D.options());
   torch::Tensor dL_dconic = torch::zeros({P, 2, 2}, means3D.options());
   torch::Tensor dL_dopacity = torch::zeros({P, 1}, means3D.options());
   torch::Tensor dL_dcov3D = torch::zeros({P, 6}, means3D.options());
+  torch::Tensor dL_ddc = torch::zeros({P, 1, 3}, means3D.options());
   torch::Tensor dL_dsh = torch::zeros({P, M, 3}, means3D.options());
   torch::Tensor dL_dscales = torch::zeros({P, 3}, means3D.options());
   torch::Tensor dL_drotations = torch::zeros({P, 4}, means3D.options());
   
   if(P != 0)
   {  
-	  CudaRasterizer::Rasterizer::backward(P, degree, M, R,
+	  CudaRasterizer::Rasterizer::backward(P, degree, M, R, B,
 	  background.contiguous().data<float>(),
 	  W, H, 
 	  means3D.contiguous().data<float>(),
+	  dc.contiguous().data<float>(),
 	  sh.contiguous().data<float>(),
 	  colors.contiguous().data<float>(),
 	  scales.data_ptr<float>(),
@@ -179,6 +210,7 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	  reinterpret_cast<char*>(geomBuffer.contiguous().data_ptr()),
 	  reinterpret_cast<char*>(binningBuffer.contiguous().data_ptr()),
 	  reinterpret_cast<char*>(imageBuffer.contiguous().data_ptr()),
+	  reinterpret_cast<char*>(sampleBuffer.contiguous().data_ptr()),
 	  dL_dout_color.contiguous().data<float>(),
 	  dL_dmeans2D.contiguous().data<float>(),
 	  dL_dconic.contiguous().data<float>(),  
@@ -186,13 +218,14 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	  dL_dcolors.contiguous().data<float>(),
 	  dL_dmeans3D.contiguous().data<float>(),
 	  dL_dcov3D.contiguous().data<float>(),
+	  dL_ddc.contiguous().data<float>(),
 	  dL_dsh.contiguous().data<float>(),
 	  dL_dscales.contiguous().data<float>(),
 	  dL_drotations.contiguous().data<float>(),
 	  debug);
   }
 
-  return std::make_tuple(dL_dmeans2D, dL_dcolors, dL_dopacity, dL_dmeans3D, dL_dcov3D, dL_dsh, dL_dscales, dL_drotations);
+  return std::make_tuple(dL_dmeans2D, dL_dcolors, dL_dopacity, dL_dmeans3D, dL_dcov3D, dL_ddc, dL_dsh, dL_dscales, dL_drotations);
 }
 
 torch::Tensor markVisible(
@@ -214,4 +247,31 @@ torch::Tensor markVisible(
   }
   
   return present;
+}
+
+void adamUpdate(
+	torch::Tensor &param,
+	torch::Tensor &param_grad,
+	torch::Tensor &exp_avg,
+	torch::Tensor &exp_avg_sq,
+	torch::Tensor &visible,
+	const float lr,
+	const float b1,
+	const float b2,
+	const float eps,
+	const uint32_t N,
+	const uint32_t M
+){
+	ADAM::adamUpdate(
+		param.contiguous().data<float>(),
+		param_grad.contiguous().data<float>(),
+		exp_avg.contiguous().data<float>(),
+		exp_avg_sq.contiguous().data<float>(),
+		visible.contiguous().data<bool>(),
+		lr,
+		b1,
+		b2,
+		eps,
+		N,
+		M);
 }
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/rasterize_points.h b/submodules/diff-gaussian-rasterization/rasterize_points.h
index 9023d99..48da910 100644
--- a/submodules/diff-gaussian-rasterization/rasterize_points.h
+++ b/submodules/diff-gaussian-rasterization/rasterize_points.h
@@ -15,7 +15,7 @@
 #include <tuple>
 #include <string>
 	
-std::tuple<int, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
+std::tuple<int, int, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
 RasterizeGaussiansCUDA(
 	const torch::Tensor& background,
 	const torch::Tensor& means3D,
@@ -31,13 +31,14 @@ RasterizeGaussiansCUDA(
 	const float tan_fovy,
     const int image_height,
     const int image_width,
+	const torch::Tensor& dc,
 	const torch::Tensor& sh,
 	const int degree,
 	const torch::Tensor& campos,
 	const bool prefiltered,
 	const bool debug);
 
-std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
+std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor>
  RasterizeGaussiansBackwardCUDA(
  	const torch::Tensor& background,
 	const torch::Tensor& means3D,
@@ -52,6 +53,7 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	const float tan_fovx, 
 	const float tan_fovy,
     const torch::Tensor& dL_dout_color,
+	const torch::Tensor& dc,
 	const torch::Tensor& sh,
 	const int degree,
 	const torch::Tensor& campos,
@@ -59,9 +61,42 @@ std::tuple<torch::Tensor, torch::Tensor, torch::Tensor, torch::Tensor, torch::Te
 	const int R,
 	const torch::Tensor& binningBuffer,
 	const torch::Tensor& imageBuffer,
+	const int B,
+	const torch::Tensor& sampleBuffer,
 	const bool debug);
 		
 torch::Tensor markVisible(
 		torch::Tensor& means3D,
 		torch::Tensor& viewmatrix,
-		torch::Tensor& projmatrix);
\ No newline at end of file
+		torch::Tensor& projmatrix);
+
+void adamUpdate(
+	torch::Tensor &param,
+	torch::Tensor &param_grad,
+	torch::Tensor &exp_avg,
+	torch::Tensor &exp_avg_sq,
+	torch::Tensor &visible,
+	const float lr,
+	const float b1,
+	const float b2,
+	const float eps,
+	const uint32_t N,
+	const uint32_t M
+);
+	
+torch::Tensor
+fusedssim(
+    float C1,
+    float C2,
+    torch::Tensor &img1,
+    torch::Tensor &img2
+);
+
+torch::Tensor
+fusedssim_backward(
+    float C1,
+    float C2,
+    torch::Tensor &img1,
+    torch::Tensor &img2,
+    torch::Tensor &dL_dmap
+);
\ No newline at end of file
diff --git a/submodules/diff-gaussian-rasterization/setup.py b/submodules/diff-gaussian-rasterization/setup.py
index bb7220d..9a7c4f5 100644
--- a/submodules/diff-gaussian-rasterization/setup.py
+++ b/submodules/diff-gaussian-rasterization/setup.py
@@ -24,7 +24,9 @@ setup(
             "cuda_rasterizer/rasterizer_impl.cu",
             "cuda_rasterizer/forward.cu",
             "cuda_rasterizer/backward.cu",
+            "cuda_rasterizer/adam.cu",
             "rasterize_points.cu",
+            "conv.cu",
             "ext.cpp"],
             extra_compile_args={"nvcc": ["-I" + os.path.join(os.path.dirname(os.path.abspath(__file__)), "third_party/glm/")]})
         ],
diff --git a/train.py b/train.py
index 5d819b3..f5bc209 100644
--- a/train.py
+++ b/train.py
@@ -12,7 +12,7 @@
 import os
 import torch
 from random import randint
-from utils.loss_utils import l1_loss, ssim
+from utils.loss_utils import l1_loss, fast_ssim
 from gaussian_renderer import render, network_gui
 import sys
 from scene import Scene, GaussianModel
@@ -31,7 +31,7 @@ except ImportError:
 def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoint_iterations, checkpoint, debug_from):
     first_iter = 0
     tb_writer = prepare_output_and_logger(dataset)
-    gaussians = GaussianModel(dataset.sh_degree)
+    gaussians = GaussianModel(dataset.sh_degree, opt.optimizer_type)
     scene = Scene(dataset, gaussians)
     gaussians.training_setup(opt)
     if checkpoint:
@@ -44,11 +44,12 @@ def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoi
     iter_start = torch.cuda.Event(enable_timing = True)
     iter_end = torch.cuda.Event(enable_timing = True)
 
-    viewpoint_stack = None
+    viewpoint_stack = scene.getTrainCameras().copy()
+    viewpoint_indices = list(range(len(viewpoint_stack)))
     ema_loss_for_log = 0.0
     progress_bar = tqdm(range(first_iter, opt.iterations), desc="Training progress")
     first_iter += 1
-    for iteration in range(first_iter, opt.iterations + 1):        
+    for iteration in range(first_iter, opt.iterations + 1):
         if network_gui.conn == None:
             network_gui.try_connect()
         while network_gui.conn != None:
@@ -75,7 +76,10 @@ def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoi
         # Pick a random Camera
         if not viewpoint_stack:
             viewpoint_stack = scene.getTrainCameras().copy()
-        viewpoint_cam = viewpoint_stack.pop(randint(0, len(viewpoint_stack)-1))
+            viewpoint_indices = list(range(len(viewpoint_stack)))
+        rand_idx = randint(0, len(viewpoint_indices) - 1)
+        viewpoint_cam = viewpoint_stack.pop(rand_idx)
+        vind = viewpoint_indices.pop(rand_idx)
 
         # Render
         if (iteration - 1) == debug_from:
@@ -89,7 +93,9 @@ def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoi
         # Loss
         gt_image = viewpoint_cam.original_image.cuda()
         Ll1 = l1_loss(image, gt_image)
-        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim(image, gt_image))
+        ssim_value = fast_ssim(image, gt_image)
+        loss = (1.0 - opt.lambda_dssim) * Ll1 + opt.lambda_dssim * (1.0 - ssim_value)
+
         loss.backward()
 
         iter_end.record()
@@ -117,15 +123,20 @@ def training(dataset, opt, pipe, testing_iterations, saving_iterations, checkpoi
 
                 if iteration > opt.densify_from_iter and iteration % opt.densification_interval == 0:
                     size_threshold = 20 if iteration > opt.opacity_reset_interval else None
-                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold)
+                    gaussians.densify_and_prune(opt.densify_grad_threshold, 0.005, scene.cameras_extent, size_threshold, radii)
                 
                 if iteration % opt.opacity_reset_interval == 0 or (dataset.white_background and iteration == opt.densify_from_iter):
                     gaussians.reset_opacity()
 
             # Optimizer step
             if iteration < opt.iterations:
-                gaussians.optimizer.step()
-                gaussians.optimizer.zero_grad(set_to_none = True)
+                if opt.optimizer_type == "default":
+                    gaussians.optimizer.step()
+                    gaussians.optimizer.zero_grad(set_to_none = True)
+                elif opt.optimizer_type == "sparse_adam":
+                    visible = radii > 0
+                    gaussians.optimizer.step(visible, radii.shape[0])
+                    gaussians.optimizer.zero_grad(set_to_none = True)
 
             if (iteration in checkpoint_iterations):
                 print("\n[ITER {}] Saving Checkpoint".format(iteration))
diff --git a/utils/loss_utils.py b/utils/loss_utils.py
index 9defc23..ae64dc1 100644
--- a/utils/loss_utils.py
+++ b/utils/loss_utils.py
@@ -13,6 +13,26 @@ import torch
 import torch.nn.functional as F
 from torch.autograd import Variable
 from math import exp
+from diff_gaussian_rasterization._C import fusedssim, fusedssim_backward
+
+C1 = 0.01 ** 2
+C2 = 0.03 ** 2
+
+class FusedSSIMMap(torch.autograd.Function):
+    @staticmethod
+    def forward(ctx, C1, C2, img1, img2):
+        ssim_map = fusedssim(C1, C2, img1, img2)
+        ctx.save_for_backward(img1.detach(), img2)
+        ctx.C1 = C1
+        ctx.C2 = C2
+        return ssim_map
+
+    @staticmethod
+    def backward(ctx, opt_grad):
+        img1, img2 = ctx.saved_tensors
+        C1, C2 = ctx.C1, ctx.C2
+        grad = fusedssim_backward(C1, C2, img1, img2, opt_grad)
+        return None, None, grad, None
 
 def l1_loss(network_output, gt):
     return torch.abs((network_output - gt)).mean()
@@ -62,3 +82,7 @@ def _ssim(img1, img2, window, window_size, channel, size_average=True):
     else:
         return ssim_map.mean(1).mean(1).mean(1)
 
+
+def fast_ssim(img1, img2):
+    ssim_map = FusedSSIMMap.apply(C1, C2, img1, img2)
+    return ssim_map.mean()
\ No newline at end of file
